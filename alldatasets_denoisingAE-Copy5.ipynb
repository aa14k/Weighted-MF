{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:16:50.471165Z",
     "iopub.status.busy": "2024-06-18T23:16:50.470735Z",
     "iopub.status.idle": "2024-06-18T23:16:50.474702Z",
     "shell.execute_reply": "2024-06-18T23:16:50.474221Z",
     "shell.execute_reply.started": "2024-06-18T23:16:50.471136Z"
    },
    "nterop": {
     "id": "1"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] =  42\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:16:50.607471Z",
     "iopub.status.busy": "2024-06-18T23:16:50.606981Z",
     "iopub.status.idle": "2024-06-18T23:16:51.023746Z",
     "shell.execute_reply": "2024-06-18T23:16:51.023123Z",
     "shell.execute_reply.started": "2024-06-18T23:16:50.607452Z"
    },
    "nterop": {
     "id": "2"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#\n",
    "import seaborn as sn\n",
    "#sn.set()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:16:51.024910Z",
     "iopub.status.busy": "2024-06-18T23:16:51.024664Z",
     "iopub.status.idle": "2024-06-18T23:16:51.210708Z",
     "shell.execute_reply": "2024-06-18T23:16:51.210098Z",
     "shell.execute_reply.started": "2024-06-18T23:16:51.024895Z"
    },
    "nterop": {
     "id": "3"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:16:51.211943Z",
     "iopub.status.busy": "2024-06-18T23:16:51.211676Z",
     "iopub.status.idle": "2024-06-18T23:16:51.421704Z",
     "shell.execute_reply": "2024-06-18T23:16:51.421071Z",
     "shell.execute_reply.started": "2024-06-18T23:16:51.211927Z"
    },
    "nterop": {
     "id": "4"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pkl(obj, filename ):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL )\n",
    "    \n",
    "def load_pkl(filename ):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:16:51.519034Z",
     "iopub.status.busy": "2024-06-18T23:16:51.518494Z",
     "iopub.status.idle": "2024-06-18T23:16:51.600106Z",
     "shell.execute_reply": "2024-06-18T23:16:51.599505Z",
     "shell.execute_reply.started": "2024-06-18T23:16:51.519015Z"
    },
    "nterop": {
     "id": "5"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### change `DATA_DIR` to the location where movielens-20m dataset sits### cha \n",
    "DATA_DIR = '/efs/users/hsteck/public/data_for_ease/movielens20mio/'\n",
    "#DATA_DIR = '/root/projects/data/netflix_prize_data/download/'\n",
    "#msd data in dawens paper\n",
    "#DATA_DIR = '/root/projects/data/echonest_taste/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:16:52.434664Z",
     "iopub.status.busy": "2024-06-18T23:16:52.434281Z",
     "iopub.status.idle": "2024-06-18T23:16:52.437480Z",
     "shell.execute_reply": "2024-06-18T23:16:52.437006Z",
     "shell.execute_reply.started": "2024-06-18T23:16:52.434647Z"
    },
    "nterop": {
     "id": "6"
    }
   },
   "outputs": [],
   "source": [
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "7"
    }
   },
   "source": [
    "# Training/validation data, hyperparameters\n",
    "Load the pre-processed training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:16:56.664492Z",
     "iopub.status.busy": "2024-06-18T23:16:56.664176Z",
     "iopub.status.idle": "2024-06-18T23:16:56.690632Z",
     "shell.execute_reply": "2024-06-18T23:16:56.690107Z",
     "shell.execute_reply.started": "2024-06-18T23:16:56.664475Z"
    },
    "nterop": {
     "id": "8"
    }
   },
   "outputs": [],
   "source": [
    "unique_sid = list()\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:16:56.978680Z",
     "iopub.status.busy": "2024-06-18T23:16:56.978074Z",
     "iopub.status.idle": "2024-06-18T23:16:56.981959Z",
     "shell.execute_reply": "2024-06-18T23:16:56.981475Z",
     "shell.execute_reply.started": "2024-06-18T23:16:56.978661Z"
    },
    "nterop": {
     "id": "9"
    }
   },
   "outputs": [],
   "source": [
    "def load_train_data(csv_file):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    n_users = tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, n_items))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:16:57.264136Z",
     "iopub.status.busy": "2024-06-18T23:16:57.263806Z",
     "iopub.status.idle": "2024-06-18T23:16:57.267554Z",
     "shell.execute_reply": "2024-06-18T23:16:57.267107Z",
     "shell.execute_reply.started": "2024-06-18T23:16:57.264117Z"
    },
    "nterop": {
     "id": "10"
    }
   },
   "outputs": [],
   "source": [
    "def load_xtx_binary():\n",
    "    train_data = load_train_data(os.path.join(pro_dir, 'train.csv'))\n",
    "    X=train_data\n",
    "    ####normalize users\n",
    "    #nn=np.array(np.sum(X,axis=1)) [:,0]\n",
    "    #nn=1.0/np.sqrt(nn)  # user weight normalized on diagonal, approx prop to nn  (off diag)\n",
    "    #X=  sparse.spdiags(nn, 0, len(nn), len(nn)) * X\n",
    "    ### remove mean  --> cov\n",
    "    print (X.shape)\n",
    "    XtX=np.array(X.T.dot(X).todense()) \n",
    "\n",
    "    return [XtX.astype('float32'), X.shape[0] , X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:16:57.641346Z",
     "iopub.status.busy": "2024-06-18T23:16:57.640749Z",
     "iopub.status.idle": "2024-06-18T23:17:07.199127Z",
     "shell.execute_reply": "2024-06-18T23:17:07.198188Z",
     "shell.execute_reply.started": "2024-06-18T23:16:57.641327Z"
    },
    "nterop": {
     "id": "11"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116677, 20108)\n"
     ]
    }
   ],
   "source": [
    "XtX, userCnt , X =load_xtx_binary()\n",
    "XtXdiag=deepcopy(np.diag(XtX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:17:07.200901Z",
     "iopub.status.busy": "2024-06-18T23:17:07.200546Z",
     "iopub.status.idle": "2024-06-18T23:17:07.205489Z",
     "shell.execute_reply": "2024-06-18T23:17:07.205027Z",
     "shell.execute_reply.started": "2024-06-18T23:17:07.200883Z"
    },
    "nterop": {
     "id": "12"
    }
   },
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:17:07.206029Z",
     "iopub.status.busy": "2024-06-18T23:17:07.205910Z",
     "iopub.status.idle": "2024-06-18T23:17:07.547779Z",
     "shell.execute_reply": "2024-06-18T23:17:07.547135Z",
     "shell.execute_reply.started": "2024-06-18T23:17:07.206017Z"
    },
    "nterop": {
     "id": "13"
    }
   },
   "outputs": [],
   "source": [
    "test_data_tr, test_data_te = load_tr_te_data(\n",
    "    os.path.join(pro_dir, 'test_tr.csv'),\n",
    "    os.path.join(pro_dir, 'test_te.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:17:07.549323Z",
     "iopub.status.busy": "2024-06-18T23:17:07.549033Z",
     "iopub.status.idle": "2024-06-18T23:17:07.564931Z",
     "shell.execute_reply": "2024-06-18T23:17:07.564462Z",
     "shell.execute_reply.started": "2024-06-18T23:17:07.549306Z"
    },
    "nterop": {
     "id": "14"
    }
   },
   "outputs": [],
   "source": [
    "N_test = test_data_tr.shape[0]\n",
    "idxlist_test = range(N_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "15"
    }
   },
   "source": [
    "Evaluate function: Normalized discounted cumulative gain (NDCG@k) and Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:17:07.565515Z",
     "iopub.status.busy": "2024-06-18T23:17:07.565393Z",
     "iopub.status.idle": "2024-06-18T23:17:07.681538Z",
     "shell.execute_reply": "2024-06-18T23:17:07.680916Z",
     "shell.execute_reply.started": "2024-06-18T23:17:07.565502Z"
    },
    "nterop": {
     "id": "16"
    }
   },
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:17:07.682529Z",
     "iopub.status.busy": "2024-06-18T23:17:07.682276Z",
     "iopub.status.idle": "2024-06-18T23:17:07.929331Z",
     "shell.execute_reply": "2024-06-18T23:17:07.928702Z",
     "shell.execute_reply.started": "2024-06-18T23:17:07.682513Z"
    },
    "nterop": {
     "id": "17"
    }
   },
   "outputs": [],
   "source": [
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:17:07.930368Z",
     "iopub.status.busy": "2024-06-18T23:17:07.930093Z",
     "iopub.status.idle": "2024-06-18T23:17:08.033002Z",
     "shell.execute_reply": "2024-06-18T23:17:08.032380Z",
     "shell.execute_reply.started": "2024-06-18T23:17:07.930352Z"
    },
    "nterop": {
     "id": "18"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(BB):\n",
    "    #evaluate in batches\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    #makeSparseFormat(BB, 0.0)\n",
    "\n",
    "\n",
    "    batch_size_test=5000\n",
    "    n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "\n",
    "\n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        Xtest = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "        print (str(st_idx)+' ... '+str(end_idx))\n",
    "        if sparse.isspmatrix(Xtest):\n",
    "            Xtest = Xtest.toarray()\n",
    "        Xtest = Xtest.astype('float32')\n",
    "\n",
    "        #pred_val = Xtest.dot(BB_excl)\n",
    "        #pred_val = (((Xtest-mu) * scaling).dot(BBth) / scaling) +mu   # no bias\n",
    "        #pred_val = Xtest.dot(beta_0d)  # no bias\n",
    "        #pred_val =Xtest.dot(beta_lowrank)  \n",
    "        pred_val =Xtest.dot(BB)  \n",
    "\n",
    "        # exclude examples from training and validation (if any)\n",
    "        pred_val[Xtest.nonzero()] = -np.inf\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "        #calc_coverageCounts(coverageCounts2, pred_val)\n",
    "        #break  # do only 5000 users\n",
    "\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "    print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "    print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "    return [np.mean(n100_list), np.mean(r20_list), np.mean(r50_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "19"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "20"
    }
   },
   "source": [
    "# train 0 diag model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:17:08.034014Z",
     "iopub.status.busy": "2024-06-18T23:17:08.033750Z",
     "iopub.status.idle": "2024-06-18T23:17:25.906343Z",
     "shell.execute_reply": "2024-06-18T23:17:25.905492Z",
     "shell.execute_reply.started": "2024-06-18T23:17:08.033999Z"
    },
    "nterop": {
     "id": "21"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 23:17:08.155311\n",
      "2024-06-18 23:17:25.902687\n"
     ]
    }
   ],
   "source": [
    "# zero diag, full rank\n",
    "print(datetime.datetime.now())\n",
    "L2reg=300.0\n",
    "pdrop=0.125\n",
    "boost= pdrop/(1.0-pdrop)\n",
    "diagStrength =1.0\n",
    "ii_diag=np.diag_indices(XtX.shape[0])\n",
    "XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag \n",
    "BB=np.linalg.inv(XtX)\n",
    "eta = 1.0 / np.diag(BB)\n",
    "eta = L2reg+boost*XtXdiag + (eta - L2reg-boost*XtXdiag)*diagStrength\n",
    "BB*=- eta\n",
    "BB[ii_diag]=0.0 #incorrect diag, but irrelevant\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:17:25.908339Z",
     "iopub.status.busy": "2024-06-18T23:17:25.908001Z",
     "iopub.status.idle": "2024-06-18T23:17:34.717092Z",
     "shell.execute_reply": "2024-06-18T23:17:34.716394Z",
     "shell.execute_reply.started": "2024-06-18T23:17:25.908319Z"
    },
    "nterop": {
     "id": "22"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 23:17:25.909285\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.42245 (0.00215)\n",
      "Test Recall@20=0.39232 (0.00268)\n",
      "Test Recall@50=0.52273 (0.00282)\n",
      "2024-06-18 23:17:34.710133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4224484206963344, 0.3923214242839204, 0.5227278221083802]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:17:34.718240Z",
     "iopub.status.busy": "2024-06-18T23:17:34.718016Z",
     "iopub.status.idle": "2024-06-18T23:18:53.918370Z",
     "shell.execute_reply": "2024-06-18T23:18:53.917519Z",
     "shell.execute_reply.started": "2024-06-18T23:17:34.718223Z"
    },
    "nterop": {
     "id": "23"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================\n",
      "2024-06-18 23:17:34.720774\n",
      "2024-06-18 23:17:52.311051\n",
      "2024-06-18 23:17:52.311357\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.42304 (0.00214)\n",
      "Test Recall@20=0.39101 (0.00268)\n",
      "Test Recall@50=0.52363 (0.00282)\n",
      "2024-06-18 23:18:01.010673\n",
      "2024-06-18 23:18:01.013015\n",
      "===================================================================\n",
      "2024-06-18 23:18:01.013033\n",
      "2024-06-18 23:18:18.856044\n",
      "2024-06-18 23:18:18.856341\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.42267 (0.00214)\n",
      "Test Recall@20=0.39086 (0.00268)\n",
      "Test Recall@50=0.52322 (0.00282)\n",
      "2024-06-18 23:18:27.599002\n",
      "2024-06-18 23:18:27.601343\n",
      "===================================================================\n",
      "2024-06-18 23:18:27.601380\n",
      "2024-06-18 23:18:45.139126\n",
      "2024-06-18 23:18:45.139424\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.42228 (0.00214)\n",
      "Test Recall@20=0.39082 (0.00268)\n",
      "Test Recall@50=0.52293 (0.00282)\n",
      "2024-06-18 23:18:53.913245\n",
      "2024-06-18 23:18:53.915464\n"
     ]
    }
   ],
   "source": [
    "# zero diag, full rank\n",
    "L2reg=700.0\n",
    "pdrop=0.125\n",
    "diagStrength =1.0\n",
    "\n",
    "for pdrop in [0.2, 0.15, 0.125]:\n",
    "    print(\"===================================================================\")\n",
    "    print(datetime.datetime.now())\n",
    "    boost= pdrop/(1.0-pdrop)\n",
    "    ii_diag=np.diag_indices(XtX.shape[0])\n",
    "    XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag \n",
    "    BB=np.linalg.inv(XtX)\n",
    "    eta = 1.0 / np.diag(BB)\n",
    "    eta = L2reg+boost*XtXdiag + (eta - L2reg-boost*XtXdiag)*diagStrength\n",
    "    BB*=- eta\n",
    "    BB[ii_diag]=0.0 #incorrect diag, but irrelevant\n",
    "    print(datetime.datetime.now())\n",
    "    evaluate(BB)\n",
    "    print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "24"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:19:23.998719Z",
     "iopub.status.busy": "2024-06-18T23:19:23.998305Z",
     "iopub.status.idle": "2024-06-18T23:19:24.007048Z",
     "shell.execute_reply": "2024-06-18T23:19:24.006455Z",
     "shell.execute_reply.started": "2024-06-18T23:19:23.998700Z"
    },
    "nterop": {
     "id": "25"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3717840567.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_16561/3717840567.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    -------------------------------------- boost 0.3\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "nflx\n",
    "\n",
    "optimal L2 \n",
    "\n",
    "-------------------------------------- boost 0.3\n",
    "-- L2=1000  and  boost= 0.3\n",
    "Test NDCG@100=0.39744 (0.00099)\n",
    "Test Recall@20=0.36504 (0.00128)\n",
    "Test Recall@50=0.44913 (0.00124)\n",
    "\n",
    "-- L2=700  and  boost= 0.3\n",
    "Test NDCG@100=0.39754 (0.00099)\n",
    "Test Recall@20=0.36533 (0.00128)\n",
    "Test Recall@50=0.44927 (0.00124)\n",
    "\n",
    "-- L2=300  and  boost= 0.3\n",
    "Test NDCG@100=0.39737 (0.00099)\n",
    "Test Recall@20=0.36524 (0.00128)\n",
    "Test Recall@50=0.44873 (0.00124)\n",
    "\n",
    "---------------------------------------boost 0.5\n",
    "-- L2=700  and  boost= 0.5\n",
    "Test NDCG@100=0.39767 (0.00099)\n",
    "Test Recall@20=0.36536 (0.00128)\n",
    "Test Recall@50=0.44923 (0.00124)\n",
    "\n",
    "-- L2=300  and  boost= 0.5\n",
    "Test NDCG@100=0.39753 (0.00099)\n",
    "Test Recall@20=0.36533 (0.00128)\n",
    "Test Recall@50=0.44907 (0.00124)\n",
    "\n",
    "--------------------------------------boost 1.0\n",
    "\n",
    "-- L2=700  and  boost= 1.0\n",
    "Test NDCG@100=0.39536 (0.00098)\n",
    "Test Recall@20=0.36269 (0.00127)\n",
    "Test Recall@50=0.44700 (0.00124)\n",
    "\n",
    "-- L2=300  and  boost= 1.0\n",
    "Test NDCG@100=0.39551 (0.00098)\n",
    "Test Recall@20=0.36272 (0.00127)\n",
    "Test Recall@50=0.44697 (0.00124)\n",
    "\n",
    "=================================== L2=0\n",
    "-- L2=0  and  boost= 0.3\n",
    "Test NDCG@100=0.39091 (0.00100)\n",
    "Test Recall@20=0.35941 (0.00127)\n",
    "Test Recall@50=0.44085 (0.00125)\n",
    "\n",
    "\n",
    "-- L2=0  and  boost= 0.5\n",
    "Test NDCG@100=0.39264 (0.00100)\n",
    "Test Recall@20=0.36054 (0.00127)\n",
    "Test Recall@50=0.44276 (0.00125)\n",
    "\n",
    "-- L2=0  and  boost= 1.0\n",
    "Test NDCG@100=0.39256 (0.00099)\n",
    "Test Recall@20=0.35996 (0.00127)\n",
    "Test Recall@50=0.44324 (0.00124)\n",
    "\n",
    "\n",
    "-- L2=0  and  boost= 2.0\n",
    "Test NDCG@100=0.38765 (0.00097)\n",
    "Test Recall@20=0.35395 (0.00126)\n",
    "Test Recall@50=0.43822 (0.00124)\n",
    "\n",
    "-- L2=0  and  boost= 10.0\n",
    "Test NDCG@100=0.35408 (0.00093)\n",
    "Test Recall@20=0.32016 (0.00122)\n",
    "Test Recall@50=0.40299 (0.00123)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "26"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "27"
    }
   },
   "source": [
    "# train denoising AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:20:32.945214Z",
     "iopub.status.busy": "2024-06-18T23:20:32.944786Z",
     "iopub.status.idle": "2024-06-18T23:20:32.949415Z",
     "shell.execute_reply": "2024-06-18T23:20:32.948950Z",
     "shell.execute_reply.started": "2024-06-18T23:20:32.945194Z"
    },
    "nterop": {
     "id": "28"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.500000150000015"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_drop=0.3333334\n",
    "prob_present=1.0-prob_drop\n",
    "\n",
    "prob_drop / prob_present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:20:33.663481Z",
     "iopub.status.busy": "2024-06-18T23:20:33.663063Z",
     "iopub.status.idle": "2024-06-18T23:21:04.356203Z",
     "shell.execute_reply": "2024-06-18T23:21:04.355534Z",
     "shell.execute_reply.started": "2024-06-18T23:20:33.663463Z"
    },
    "nterop": {
     "id": "29"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "2024-06-18 23:20:33.666514\n",
      "2024-06-18 23:20:51.261806\n",
      "0.27624794840812683 .  0.21409182250499725\n",
      "---- prob_drop=0.05    L2reg=0.0     L2_drop=0.052631578947368425\n",
      "2024-06-18 23:20:55.124279\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.39417 (0.00219)\n",
      "Test Recall@20=0.36874 (0.00270)\n",
      "Test Recall@50=0.48960 (0.00294)\n",
      "2024-06-18 23:21:04.351404\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L2regList = [0.0]\n",
    "    \n",
    "for kk in range(len(L2regList)):\n",
    "    print (\"==========================================================\")\n",
    "    prob_drop=0.05\n",
    "    prob_present=1.0-prob_drop\n",
    "\n",
    "    L2reg= L2regList[kk]\n",
    "    L2_drop = prob_drop / prob_present\n",
    "\n",
    "\n",
    "    #train precompute\n",
    "    print(datetime.datetime.now())\n",
    "    ii_diag=np.diag_indices(XtX.shape[0])\n",
    "    XtX[ii_diag]=  XtXdiag *(1.0+L2_drop) + L2reg \n",
    "    CC=np.linalg.inv(XtX)\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    # train iterate\n",
    "    beta=0.0\n",
    "    for _ in range(1):\n",
    "        eta = (1.0 - prob_present *beta ) / np.diag(CC)\n",
    "        BB = CC * (-eta)\n",
    "        BB[ii_diag]= 0.0 # prob_present *beta\n",
    "        beta=   1.0-  np.diag(  XtX.dot(BB)  )   / XtXdiag\n",
    "        print(\"{} .  {}\".format(np.mean(beta), np.std(beta)))\n",
    "\n",
    "    #evaluate\n",
    "    print(\"---- prob_drop={}    L2reg={}     L2_drop={}\".format(prob_drop, L2reg, L2_drop))\n",
    "    evaluate(BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T23:21:04.357747Z",
     "iopub.status.busy": "2024-06-18T23:21:04.357399Z",
     "iopub.status.idle": "2024-06-18T23:21:04.361384Z",
     "shell.execute_reply": "2024-06-18T23:21:04.360777Z",
     "shell.execute_reply.started": "2024-06-18T23:21:04.357728Z"
    },
    "nterop": {
     "id": "30"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2487073303.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_16561/2487073303.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ====== pop drop =0.3333333333\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "====== pop drop =0.3333333333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T23:21:04.361687Z",
     "iopub.status.idle": "2024-06-18T23:21:04.361853Z",
     "shell.execute_reply": "2024-06-18T23:21:04.361777Z",
     "shell.execute_reply.started": "2024-06-18T23:21:04.361769Z"
    },
    "nterop": {
     "id": "31"
    }
   },
   "outputs": [],
   "source": [
    "beta_closedform= 1.0/prob_present  * ( 1.0- prob_drop/prob_present * XtXdiag * np.diag(CC)   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T23:21:04.362516Z",
     "iopub.status.idle": "2024-06-18T23:21:04.362672Z",
     "shell.execute_reply": "2024-06-18T23:21:04.362600Z",
     "shell.execute_reply.started": "2024-06-18T23:21:04.362592Z"
    },
    "nterop": {
     "id": "32"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(beta, beta_closedform,'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T23:21:04.363416Z",
     "iopub.status.idle": "2024-06-18T23:21:04.363576Z",
     "shell.execute_reply": "2024-06-18T23:21:04.363497Z",
     "shell.execute_reply.started": "2024-06-18T23:21:04.363489Z"
    },
    "nterop": {
     "id": "33"
    }
   },
   "outputs": [],
   "source": [
    "vec=XtXdiag * np.diag(CC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T23:21:04.364207Z",
     "iopub.status.idle": "2024-06-18T23:21:04.364368Z",
     "shell.execute_reply": "2024-06-18T23:21:04.364291Z",
     "shell.execute_reply.started": "2024-06-18T23:21:04.364284Z"
    },
    "nterop": {
     "id": "34"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(vec, bins=200, alpha=0.8,  density=True,histtype='step', color='black', linewidth=2.0,  label='calibrated')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "35"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(XtXdiag) , np.log(vec),'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "36"
    }
   },
   "outputs": [],
   "source": [
    "============ drop pop =0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "37"
    }
   },
   "outputs": [],
   "source": [
    "beta_closedform= 1.0/prob_present  * ( 1.0- prob_drop/prob_present * XtXdiag * np.diag(CC)   )\n",
    "\n",
    "plt.plot(beta, beta_closedform,'x')\n",
    "plt.show()\n",
    "\n",
    "vec=XtXdiag * np.diag(CC)\n",
    "plt.hist(vec, bins=200, alpha=0.8,  density=True,histtype='step', color='black', linewidth=2.0,  label='calibrated')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "38"
    }
   },
   "outputs": [],
   "source": [
    "beta_closedform_075=deepcopy(beta_closedform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "39"
    }
   },
   "outputs": [],
   "source": [
    "beta_closedform_033=deepcopy(beta_closedform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "40"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(beta_closedform_033,beta_closedform_075,'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "41"
    }
   },
   "outputs": [],
   "source": [
    "beta_closedform_01= 1.0/prob_present  * ( 1.0- prob_drop/prob_present * XtXdiag * np.diag(CC)   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "42"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(beta_closedform_033,beta_closedform_01,'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "43"
    }
   },
   "outputs": [],
   "source": [
    "beta_closedform_005= 1.0/prob_present  * ( 1.0- prob_drop/prob_present * XtXdiag * np.diag(CC)   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "44"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(beta_closedform_01,beta_closedform_005,'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "45"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "46"
    }
   },
   "outputs": [],
   "source": [
    "del beta_closedform_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "47"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "48"
    }
   },
   "outputs": [],
   "source": [
    "---- prob_drop=0.5    L2reg=700.0     L2_drop=1.0\n",
    "Test NDCG@100=0.39211 (0.00098)\n",
    "Test Recall@20=0.35892 (0.00127)\n",
    "Test Recall@50=0.44253 (0.00124)\n",
    "---- prob_drop=0.5    L2reg=200.0     L2_drop=1.0   ... best L2\n",
    "Test NDCG@100=0.39241 (0.00098)\n",
    "Test Recall@20=0.35962 (0.00127)\n",
    "Test Recall@50=0.44256 (0.00124)\n",
    "---- prob_drop=0.5    L2reg=100.0     L2_drop=1.0\n",
    "Test NDCG@100=0.39208 (0.00098)\n",
    "Test Recall@20=0.35928 (0.00127)\n",
    "Test Recall@50=0.44204 (0.00124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "49"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "50"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "51"
    }
   },
   "outputs": [],
   "source": [
    "nflx\n",
    "---drop 0.3\n",
    "0.8734440207481384 .  0.07434113323688507\n",
    "Test NDCG@100=0.38795 (0.00099)\n",
    "Test Recall@20=0.35616 (0.00128)\n",
    "Test Recall@50=0.43691 (0.0012)\n",
    "\n",
    "-- drop 0.5\n",
    "0.8447370529174805 .  0.08548472821712494\n",
    "Test NDCG@100=0.38996 (0.00099)\n",
    "Test Recall@20=0.35728 (0.00127)\n",
    "Test Recall@50=0.43953 (0.00124)\n",
    "\n",
    "\n",
    "-- drop 0.7\n",
    "Test NDCG@100=0.38360 (0.00097)\n",
    "Test Recall@20=0.34970 (0.00126)\n",
    "Test Recall@50=0.43369 (0.00124)\n",
    "\n",
    "-- drop 0.7 with l2_drop=0.5\n",
    "Test NDCG@100=0.39155 (0.00099)\n",
    "Test Recall@20=0.35898 (0.00127)\n",
    "Test Recall@50=0.44142 (0.00124)\n",
    "\n",
    "-- drop 0.999 with optimal l2: 700 and ratio=0.5\n",
    "Test NDCG@100=0.39767 (0.00099)\n",
    "Test Recall@20=0.36536 (0.00128)\n",
    "Test Recall@50=0.44923 (0.00124)\n",
    "\n",
    "-- drop 0.7 with optimal l2: 700 and ratio=0.5\n",
    "Test NDCG@100=0.39661 (0.00099)\n",
    "Test Recall@20=0.36417 (0.00128)\n",
    "Test Recall@50=0.44775 (0.00124)\n",
    "    \n",
    "-- drop 0.5 with optimal l2: 700 and ratio=0.5    ... also a regular dropout with addl L2 700\n",
    "Test NDCG@100=0.39510 (0.00099)\n",
    "Test Recall@20=0.36292 (0.00128)\n",
    "Test Recall@50=0.44599 (0.00124)\n",
    "\n",
    "-- drop 0.25 with optimal l2: 700 and ratio=0.5\n",
    "Test NDCG@100=0.39011 (0.00098)\n",
    "Test Recall@20=0.35725 (0.00128)\n",
    "Test Recall@50=0.44089 (0.00124)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "52"
    }
   },
   "outputs": [],
   "source": [
    "beta=1.0-  np.diag(  XtX.dot(BB)  )   / XtXdiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "53"
    }
   },
   "outputs": [],
   "source": [
    "print(np.median(beta))\n",
    "print(np.mean(beta))\n",
    "print(np.max(beta))\n",
    "print(np.min(beta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "54"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(XtXdiag), beta,'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "55"
    }
   },
   "outputs": [],
   "source": [
    "del BB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "56"
    }
   },
   "source": [
    "# low rank with 0diag and rho-admm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "57"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "rhoadd=200\n",
    "\n",
    "#print(\"precomputing\")\n",
    "#precompute\n",
    "#ii_diag=np.diag_indices(XtX.shape[0])\n",
    "#XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag \n",
    "#PP=np.linalg.inv(XtX)\n",
    "\n",
    "for prob_dropout in [  0.1]:\n",
    "    boost= prob_dropout/(1.0-prob_dropout)\n",
    "    print(\"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb prob_dropout= {}\".format(prob_dropout))\n",
    "    for L2reg in [ 50.0 ]:\n",
    "      for dim in  [   1000 ] : #[ 10 , 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]:\n",
    "        print(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa dim={}\".format(dim))\n",
    "\n",
    "        print(datetime.datetime.now())\n",
    "        EE=np.random.randn(XtX.shape[0], dim) * 0.0001 #* sparsityMask\n",
    "        FFt=np.zeros((dim,XtX.shape[0]),dtype=float) \n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        print(\"precomputing\")\n",
    "        #precompute\n",
    "        rhoVec = L2reg+boost*XtXdiag + rhoadd\n",
    "        ii_diag=np.diag_indices(XtX.shape[0])\n",
    "        XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag +rhoVec\n",
    "        PP=np.linalg.inv(XtX)\n",
    "\n",
    "\n",
    "        for itercnt in range(40):\n",
    "            print(\"================= iterCnt: {}\".format(itercnt))\n",
    "            #print(\"  --- train FF\")\n",
    "            #print(datetime.datetime.now())\n",
    "            ii_diag=np.diag_indices(XtX.shape[0])\n",
    "            XtX[ii_diag]= L2reg+boost*XtXdiag  +XtXdiag \n",
    "            HH=EE.T.dot(XtX).dot(EE)\n",
    "            #print(\"          10\")\n",
    "            HH= np.linalg.inv(HH)  .dot(EE.T)\n",
    "            #print(\"          11\")\n",
    "            XtX[ii_diag]= XtXdiag\n",
    "            GG= HH.dot(XtX + (rhoVec[:,None]*EE).dot(FFt)  )\n",
    "            #print(\"          12\")\n",
    "            diag_up = np.diag(EE.dot(GG))\n",
    "            diag_down = np.diag(EE.dot(HH))\n",
    "            eta= diag_up / diag_down \n",
    "            #print(\"          13\")\n",
    "            FFt = GG - HH * eta \n",
    "            del GG\n",
    "            del HH\n",
    "            #print(\"avg. diag value1: {}\".format(np.mean(np.abs(np.diag(EE.dot(FFt))))))\n",
    "            #print(\"          14\")\n",
    "            ## make sparse\n",
    "            #FFt *= sparsityMask.T\n",
    "            #print(datetime.datetime.now())\n",
    "            #print(\"  --- train EE\")\n",
    "            #print(datetime.datetime.now())\n",
    "            #XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag \n",
    "            #HH=np.linalg.inv(XtX)\n",
    "            #print(\"          1\")\n",
    "            GG= FFt.dot(FFt.T)\n",
    "            #print(\"          2\")\n",
    "            GG=np.linalg.inv(GG)\n",
    "            GG=FFt.T.dot(GG)\n",
    "            #print(\"          3\")\n",
    "            #if diagStrength>0.0:\n",
    "            #print(\"          4\")\n",
    "            #KK = FFt.T.dot(GG).dot(FFt)\n",
    "            #print(\"          5\")\n",
    "            HH= PP.dot(  XtX+(rhoVec[:,None]*EE).dot(FFt)  ).dot(GG)\n",
    "            eta= np.linalg.solve( PP * (GG.dot(FFt))  , np.diag( HH.dot(FFt)    ))\n",
    "            #eta= np.linalg.pinv(HH * KK ).dot(np.diag(KK))\n",
    "            #print(\"          6\")\n",
    "            #eta= L2reg  +boost*XtXdiag+ (eta-L2reg-boost*XtXdiag)*diagStrength\n",
    "            #print(\"          7\")\n",
    "            #else:\n",
    "            #if diagStrength==0.0:\n",
    "            #        eta= L2reg +boost*XtXdiag\n",
    "            EE = HH -  (PP *eta).dot(GG) \n",
    "            del GG\n",
    "            del HH\n",
    "            #print(\"avg. diag value2: {}\".format(np.mean(np.abs(np.diag(EE.dot(FFt))))))\n",
    "            print(datetime.datetime.now())\n",
    "            ###### eval\n",
    "            if (itercnt+1) in [1,5,10,15,20,30,40]:\n",
    "                print(\"========================= eval:\")\n",
    "                BB= EE.dot(FFt)\n",
    "                evaluate(BB)\n",
    "                del BB\n",
    "                #print(\"%d %d\\t%.3f\\t%.3f\\t%.3f\" %(dim, L2reg, np.mean(n100_list), np.mean(r20_list), np.mean(r50_list)))\n",
    "\n",
    "                print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "58"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "59"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "60"
    }
   },
   "source": [
    "# low-rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "61"
    }
   },
   "outputs": [],
   "source": [
    "########## orig old\n",
    "\n",
    "diagStrength =0.0\n",
    "boost=0.00\n",
    "\n",
    "#print(\"precomputing\")\n",
    "#precompute\n",
    "#ii_diag=np.diag_indices(XtX.shape[0])\n",
    "#XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag \n",
    "#PP=np.linalg.inv(XtX)\n",
    "\n",
    "for prob_dropout in [  0.66666667]:\n",
    "    boost= prob_dropout/(1.0-prob_dropout)\n",
    "    print(\"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb prob_dropout= {}\".format(prob_dropout))\n",
    "    for L2reg in [ 200.0 ]:\n",
    "      for dim in  [   1000 ] : #[ 10 , 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]:\n",
    "        print(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa dim={}\".format(dim))\n",
    "\n",
    "        print(datetime.datetime.now())\n",
    "        EE=np.random.randn(XtX.shape[0], dim) * 0.0001 #* sparsityMask\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        print(\"precomputing\")\n",
    "        #precompute\n",
    "        ii_diag=np.diag_indices(XtX.shape[0])\n",
    "        XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag \n",
    "        PP=np.linalg.inv(XtX)\n",
    "\n",
    "\n",
    "        for itercnt in range(40):\n",
    "            print(\"================= iterCnt: {}\".format(itercnt))\n",
    "            #print(\"  --- train FF\")\n",
    "            #print(datetime.datetime.now())\n",
    "            ii_diag=np.diag_indices(XtX.shape[0])\n",
    "            XtX[ii_diag]= L2reg+boost*XtXdiag  +XtXdiag \n",
    "            HH=EE.T.dot(XtX).dot(EE)\n",
    "            #print(\"          10\")\n",
    "            HH= np.linalg.inv(HH)  .dot(EE.T)\n",
    "            #print(\"          11\")\n",
    "            XtX[ii_diag]= XtXdiag\n",
    "            GG= HH.dot(XtX )\n",
    "            #print(\"          12\")\n",
    "            diag_up = np.diag(EE.dot(GG))\n",
    "            diag_down = np.diag(EE.dot(HH))\n",
    "            eta= diag_up / diag_down \n",
    "            #print(\"          13\")\n",
    "            FFt = GG - HH * eta * diagStrength\n",
    "            del GG\n",
    "            #print(\"          14\")\n",
    "            ## make sparse\n",
    "            #FFt *= sparsityMask.T\n",
    "            #print(datetime.datetime.now())\n",
    "            #print(\"  --- train EE\")\n",
    "            #print(datetime.datetime.now())\n",
    "            #XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag \n",
    "            #HH=np.linalg.inv(XtX)\n",
    "            #print(\"          1\")\n",
    "            GG= FFt.dot(FFt.T)\n",
    "            #print(\"          2\")\n",
    "            GG=np.linalg.inv(GG)\n",
    "            #print(\"          3\")\n",
    "            #if diagStrength>0.0:\n",
    "            #print(\"          4\")\n",
    "            #KK = FFt.T.dot(GG).dot(FFt)\n",
    "            #print(\"          5\")\n",
    "            \n",
    "            eta= L2reg +boost*XtXdiag #if diag unconstraint\n",
    "            if itercnt>3:  \n",
    "             if diagStrength>0.0:\n",
    "                print(\"------------------------------- zero diagonsl enforced !\")\n",
    "                KK = FFt.T.dot(GG).dot(FFt)\n",
    "                eta= np.linalg.solve( PP * KK   , np.diag(KK))\n",
    "            #eta= np.linalg.pinv(HH * KK ).dot(np.diag(KK))\n",
    "            #print(\"          6\")\n",
    "            #eta= L2reg  +boost*XtXdiag+ (eta-L2reg-boost*XtXdiag)*diagStrength\n",
    "            #print(\"          7\")\n",
    "            #else:\n",
    "            #if diagStrength==0.0:\n",
    "            #        eta= L2reg +boost*XtXdiag\n",
    "            HH=PP *(-eta) \n",
    "            HH[ii_diag]+=1\n",
    "            #print(\"          8\")\n",
    "            EE=HH.dot( FFt.T.dot(GG) )\n",
    "            #print(\"          9\")\n",
    "\n",
    "            ## make sparse\n",
    "            #EE*=sparsityMask\n",
    "            print(datetime.datetime.now())\n",
    "            ###### eval\n",
    "            if (itercnt+1) in [1,5,10,15,20,30,40]:\n",
    "                print(\"========================= eval:\")\n",
    "                BB= EE.dot(FFt)\n",
    "                evaluate(BB)\n",
    "                del BB\n",
    "                #print(\"%d %d\\t%.3f\\t%.3f\\t%.3f\" %(dim, L2reg, np.mean(n100_list), np.mean(r20_list), np.mean(r50_list)))\n",
    "\n",
    "                print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "62"
    }
   },
   "outputs": [],
   "source": [
    "EEnn= (1.0 /np.sqrt(np.sum(EE*EE,1)))[:,None]  *EE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "63"
    }
   },
   "outputs": [],
   "source": [
    "FFtnn= FFt / np.sqrt(np.sum(FFt*FFt,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "64"
    }
   },
   "outputs": [],
   "source": [
    "EEnn= (1.0 /np.sqrt(np.sum(EE*EE,1)))[:,None]  *EE\n",
    "FFtnn= FFt / np.sqrt(np.sum(FFt*FFt,0))\n",
    "cosineUnconstr=np.diag(EEnn.dot(FFtnn))\n",
    "del EEnn\n",
    "del FFtnn\n",
    "plt.hist(cosineUnconstr, bins=100, alpha=0.8,  density=True,histtype='step', color='black', linewidth=2.0,  label='calibrated')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "65"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(cosineUnconstr, bins=100, alpha=0.8,  density=True,histtype='step', color='black', linewidth=2.0,  label='calibrated')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "66"
    }
   },
   "outputs": [],
   "source": [
    "save_pkl(cosineUnconstr, \"/root/projects/mySLIM/paper/msd_cosineUnconstr.pkl\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "67"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "######### NEW ######### !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "boost=0.00\n",
    "\n",
    "#print(\"precomputing\")\n",
    "#precompute\n",
    "#ii_diag=np.diag_indices(XtX.shape[0])\n",
    "#XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag \n",
    "#PP=np.linalg.inv(XtX)\n",
    "\n",
    "aaaa=1.0\n",
    "bbbb=1.0\n",
    "\n",
    "for prob_dropout in [0.25]: #, 0.3333, 0.5, 0.66667, 0.75]:\n",
    "    boost= prob_dropout/(1.0-prob_dropout)\n",
    "    print(\"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb prob_dropout= {}\".format(prob_dropout))\n",
    "    for L2reg in [ 0.0  ]:\n",
    "      for dim in  [   1000 ] : #[ 10 , 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]:\n",
    "        print(\"aaaaaaa dim={}\".format(dim))\n",
    "\n",
    "        print(datetime.datetime.now())\n",
    "        EE=np.random.randn(XtX.shape[0], dim) * 0.0001 #* sparsityMask\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        print(\"precomputing\")\n",
    "        #precompute\n",
    "        ii_diag=np.diag_indices(XtX.shape[0])\n",
    "        XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag \n",
    "        PP=np.linalg.inv(XtX)\n",
    "\n",
    "        betavec=0.0 #init\n",
    "        \n",
    "        for itercnt in range(40):\n",
    "            print(\"================= iterCnt: {}\".format(itercnt))\n",
    "            #print(\"  --- train FF\")\n",
    "            #print(datetime.datetime.now())\n",
    "            XtildeDiag = 1.0+betavec*(1.0-bbbb/(aaaa*prob_dropout+bbbb*(1.0-prob_dropout)))\n",
    "\n",
    "            ii_diag=np.diag_indices(XtX.shape[0])\n",
    "            XtX[ii_diag]= L2reg+boost*XtXdiag  +XtXdiag \n",
    "            HH=EE.T.dot(XtX).dot(EE)\n",
    "            #print(\"          10\")\n",
    "            HH= np.linalg.inv(HH)  .dot(EE.T)\n",
    "            #print(\"          11\")\n",
    "            XtX[ii_diag]= XtXdiag\n",
    "            XtildeDiag = ...\n",
    "            GG= HH.dot(XtX * XtildeDiag)\n",
    "            #print(\"          12\")\n",
    "            diag_up = np.diag(EE.dot(GG))-betavec \n",
    "            diag_down = np.diag(EE.dot(HH))\n",
    "            eta= diag_up / diag_down \n",
    "            #print(\"          13\")\n",
    "            FFt = GG - HH * eta \n",
    "            #print(\"          14\")\n",
    "            ## make sparse\n",
    "            #FFt *= sparsityMask.T\n",
    "            #print(datetime.datetime.now())\n",
    "            #print(\"  --- train EE\")\n",
    "            #print(datetime.datetime.now())\n",
    "            #XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag \n",
    "            #HH=np.linalg.inv(XtX)\n",
    "            #print(\"          1\")\n",
    "            GG= FFt.dot(FFt.T)\n",
    "            #print(\"          2\")\n",
    "            GG=np.linalg.inv(GG)\n",
    "            #print(\"          3\")\n",
    "            #if diagStrength>0.0:\n",
    "            #print(\"          4\")\n",
    "            #KK = FFt.T.dot(GG).dot(FFt)\n",
    "            #print(\"          5\")\n",
    "            ######KK = FFt.T.dot(GG).dot(FFt)\n",
    "            #####eta= np.linalg.solve( PP * KK   , np.diag(KK))  ........... or reuse old value\n",
    "            #eta= np.linalg.pinv(HH * KK ).dot(np.diag(KK))\n",
    "            #print(\"          6\")\n",
    "            #eta= L2reg  +boost*XtXdiag+ (eta-L2reg-boost*XtXdiag)*diagStrength\n",
    "            #print(\"          7\")\n",
    "            #else:\n",
    "            HH = XtX * XtildeDiag\n",
    "            ii_diag=np.diag_indices(HH.shape[0])\n",
    "            HH[ii_diag] -= eta \n",
    "            HH=PP.dot(HH)\n",
    "            #print(\"          8\")\n",
    "            EE=HH.dot( FFt.T.dot(GG) )\n",
    "            #print(\"          9\")\n",
    "\n",
    "            \n",
    "            #### \n",
    "            XtX[ii_diag]=XtXdiag\n",
    "            BB= EE.dot(FFt)\n",
    "            BB[ii_diag]=0.0\n",
    "            betavec= (1.0-prob_dropout)*(1.0- (L2reg+np.diag(XtX.dot(BB)))  / (L2reg+XtXdiag))\n",
    "            ## make sparse\n",
    "            #EE*=sparsityMask\n",
    "            print(datetime.datetime.now())\n",
    "            ###### eval\n",
    "            if (itercnt+1) in [1,5,10,15,20,30,40]:\n",
    "                print(\"========================= eval:\")\n",
    "                BB= EE.dot(FFt)\n",
    "                evaluate(BB)\n",
    "\n",
    "                #print(\"%d %d\\t%.3f\\t%.3f\\t%.3f\" %(dim, L2reg, np.mean(n100_list), np.mean(r20_list), np.mean(r50_list)))\n",
    "\n",
    "                print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "68"
    }
   },
   "outputs": [],
   "source": [
    "--- boost=1, l2=0, full diag\n",
    "Test NDCG@100=0.37212 (0.00098)\n",
    "Test Recall@20=0.34091 (0.00126)\n",
    "Test Recall@50=0.42247 (0.00124)\n",
    "\n",
    "--- boost=0.33333, l2=0, zero diag\n",
    "Test NDCG@100=0.38136 (0.00099)\n",
    "Test Recall@20=0.35009 (0.00127)\n",
    "Test Recall@50=0.42930 (0.00125)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "69"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "70"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "71"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "72"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "73"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "74"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "75"
    }
   },
   "source": [
    "# tensorflow DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "76"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import apply_regularization, l2_regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "77"
    }
   },
   "outputs": [],
   "source": [
    "def train_TF_DAE(weightsList, X,  hidden_dim, L2reg_total, dropout_prob, lam_W, train_epochs, bsize = 256):\n",
    "   \n",
    "    \n",
    "    n_dim=X.shape[1]\n",
    "    y_dim=n_dim\n",
    "\n",
    "    #defhidden_dim, \n",
    "    #dropout_prob, \n",
    "    #lam_W = 0.01  # L2 per layer\n",
    "    #bsize = 256  # batch size for training\n",
    "    # no bias terms in model\n",
    "\n",
    "    tf.reset_default_graph()  # reset graph if run many times\n",
    "    ############## input X and target y\n",
    "    # None is saved as batch_size\n",
    "    input_ph = tf.placeholder(tf.float32, shape=(None, n_dim), name='input')\n",
    "    target_ph = tf.placeholder(tf.float32, shape=(None, y_dim), name='target')\n",
    "    ############ regularization lambda\n",
    "    lam = tf.placeholder(tf.float32, shape=(), name='lambda')\n",
    "    lam_total = tf.placeholder(tf.float32, shape=(), name='lambda_total')\n",
    "    \n",
    "    ########### network structure \n",
    "    model_dims = [n_dim, hidden_dim, y_dim]\n",
    "    # weight for the neural nets, no bias terms\n",
    "    Ws = []\n",
    "    for l, (din, dout) in enumerate(zip(model_dims[:-1], model_dims[1:])):\n",
    "        print(\"Layer %d: input dimension=%d, output dimension=%d\" % (l, din, dout))\n",
    "        if len(weightsList)==0:\n",
    "            Ws.append(tf.get_variable(name=\"weight%d\" % l, shape=[din, dout], \n",
    "                                  initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        else:\n",
    "            Ws.append(tf.Variable(initial_value=weightsList[l], name=\"weight%d\" % l))          \n",
    "    # define network architecture\n",
    "    h1 = input_ph\n",
    "    h = tf.nn.dropout(h1, rate=dropout_prob)\n",
    "    for w in Ws[:-1]:\n",
    "        h = tf.matmul(h, w)   # linear model\n",
    "    y_hat = tf.matmul(h, Ws[-1])\n",
    "    ########### define regularization and loss\n",
    "    reg = l2_regularizer(lam)        \n",
    "    reg_var = apply_regularization(reg, Ws)\n",
    "\n",
    "    reg_total = l2_regularizer(lam_total)        \n",
    "    #dummy = tf.get_variable(name=\"dummy\", shape=[din, dout], \n",
    "    #                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    reg_var_total = apply_regularization(reg_total, [ tf.matmul(Ws[0],Ws[1]) ] )\n",
    "\n",
    "    mse = tf.reduce_mean( tf.square(tf.subtract(target_ph, y_hat)))\n",
    "\n",
    "    loss = mse +    reg_var_total + reg_var \n",
    "    ############# define optimizer\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    ########### start training\n",
    "    N = X.shape[0]\n",
    "    idxlist = np.arange(N)\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        loss_list = list()\n",
    "        mse_list = list()\n",
    "\n",
    "        for epoch in range(train_epochs):\n",
    "            loss_epoch = list()\n",
    "            mse_epoch = list()\n",
    "\n",
    "            np.random.shuffle(idxlist)\n",
    "            for bnum, st_idx in enumerate(range(0, N, bsize)):\n",
    "                end_idx = min(st_idx + bsize, N)\n",
    "\n",
    "                inp = X[idxlist[st_idx:end_idx]]\n",
    "                if sparse.isspmatrix(inp):\n",
    "                    inp = inp.toarray()\n",
    "                    inp = inp.astype('float32')           \n",
    "            \n",
    "                \n",
    "                \n",
    "                tar = inp\n",
    "\n",
    "                feed_dict = {input_ph:inp, target_ph:tar, lam:lam_W, lam_total:L2reg_total}\n",
    "\n",
    "                l, m, _ = sess.run([loss, mse, train_op], feed_dict=feed_dict)\n",
    "                loss_epoch.append(l)\n",
    "                mse_epoch.append(m)\n",
    "\n",
    "            loss_list.append(np.mean(loss_epoch))\n",
    "            mse_list.append(np.mean(mse_epoch))\n",
    "\n",
    "\n",
    "            print(\"Epoch: %d, training_rmse=%.3f, training_loss=%.3f\" % (epoch, np.sqrt(mse_list[-1]), loss_list[-1]))\n",
    "            print(datetime.datetime.now())\n",
    "            \n",
    " \n",
    "            if epoch+1 in [1,5,10,20,30,40,50]:\n",
    "                print(\"--epoch {}\".format(epoch))\n",
    "                UU= sess.run(Ws[0])\n",
    "                VV = sess.run(Ws[1] ) \n",
    "                BBtf= UU.dot(VV)\n",
    "                evaluate(BBtf)\n",
    "\n",
    "            \n",
    "        UU= sess.run(Ws[0])\n",
    "        VV = sess.run(Ws[1] ) \n",
    "    return [UU,VV, loss_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "78"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=500\n",
    "dropout_prob=0.0\n",
    "L2reg_total = 100000.0\n",
    "lam_W = 0.0  # L2 per layer\n",
    "bsize = 4096  # batch size for training\n",
    "train_epochs = 5\n",
    "\n",
    "L2reg_total_perbatch = bsize/1.0/userCnt * L2reg_total\n",
    "print(datetime.datetime.now())\n",
    "UUtf, VVtf, loss_list = train_TF_DAE([],X,  hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "79"
    }
   },
   "outputs": [],
   "source": [
    "L2reg_total_perbatch = bsize/1.0/userCnt * L2reg_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "80"
    }
   },
   "outputs": [],
   "source": [
    "print(UUtf.shape)\n",
    "print(VVtf.shape)\n",
    "BBtf= UUtf.dot(VVtf)\n",
    "print(BBtf.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "81"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(BBtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "82"
    }
   },
   "outputs": [],
   "source": [
    "Test NDCG@100=0.37316 (0.00099)\n",
    "Test Recall@20=0.34315 (0.00126)\n",
    "Test Recall@50=0.42084 (0.00124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "83"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=500\n",
    "dropout_prob=0.0\n",
    "L2reg_total = 100000.0\n",
    "lam_W = 0.0  # L2 per layer\n",
    "bsize = 4096  # batch size for training\n",
    "train_epochs = 5\n",
    "\n",
    "L2reg_total_perbatch = bsize/1.0/userCnt * L2reg_total\n",
    "print(datetime.datetime.now())\n",
    "UUtf, VVtf, loss_list = train_TF_DAE([],X,  hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "BBtf= UUtf.dot(VVtf)\n",
    "evaluate(BBtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "84"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=500\n",
    "dropout_prob=0.0\n",
    "L2reg_total = 100000.0\n",
    "lam_W = 0.0  # L2 per layer\n",
    "bsize = 4096  # batch size for training\n",
    "train_epochs = 5\n",
    "\n",
    "L2reg_total_perbatch = 1.0/userCnt * L2reg_total\n",
    "print(datetime.datetime.now())\n",
    "UUtf, VVtf, loss_list = train_TF_DAE([],X,  hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "BBtf= UUtf.dot(VVtf)\n",
    "evaluate(BBtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "85"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=500\n",
    "dropout_prob=0.0\n",
    "L2reg_total = 100000.0\n",
    "lam_W = 0.0  # L2 per layer\n",
    "bsize = 4096  # batch size for training\n",
    "train_epochs = 5\n",
    "\n",
    "L2reg_total_perbatch = 0.001*1.0/userCnt * L2reg_total\n",
    "print(datetime.datetime.now())\n",
    "UUtf, VVtf, loss_list = train_TF_DAE([],X,  hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "BBtf= UUtf.dot(VVtf)\n",
    "evaluate(BBtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "86"
    }
   },
   "outputs": [],
   "source": [
    "dden_dim=500\n",
    "dropout_prob=0.0\n",
    "L2reg_total = 100000.0\n",
    "lam_W = 0.0  # L2 per layer\n",
    "bsize = 4096  # batch size for training\n",
    "train_epochs = 5\n",
    "\n",
    "L2reg_total_perbatch = 0.0001*1.0/userCnt * L2reg_total\n",
    "print(datetime.datetime.now())\n",
    "UUtf, VVtf, loss_list = train_TF_DAE([],X,  hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "BBtf= UUtf.dot(VVtf)\n",
    "evaluate(BBtf)\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    print(datetime.datetime.now())\n",
    "    UUtf, VVtf, loss_list = train_TF_DAE([UUtf, VVtf],X, hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    BBtf= UUtf.dot(VVtf)\n",
    "    evaluate(BBtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "87"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=500\n",
    "dropout_prob=0.0\n",
    "L2reg_total = 100000.0\n",
    "lam_W = 0.0  # L2 per layer\n",
    "bsize = 4096  # batch size for training\n",
    "train_epochs = 5\n",
    "\n",
    "L2reg_total_perbatch = 0.0003*1.0/userCnt * L2reg_total\n",
    "print(datetime.datetime.now())\n",
    "UUtf, VVtf, loss_list = train_TF_DAE([],X,  hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "BBtf= UUtf.dot(VVtf)\n",
    "evaluate(BBtf)\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    print(datetime.datetime.now())\n",
    "    UUtf, VVtf, loss_list = train_TF_DAE([UUtf, VVtf],X, hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    BBtf= UUtf.dot(VVtf)\n",
    "    evaluate(BBtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "88"
    }
   },
   "outputs": [],
   "source": [
    "0.0001*1.0/userCnt * L2reg_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "89"
    }
   },
   "outputs": [],
   "source": [
    "dden_dim=500\n",
    "dropout_prob=0.0\n",
    "L2reg_total = 100000.0\n",
    "lam_W = 0.0  # L2 per layer\n",
    "bsize = 4096  # batch size for training\n",
    "train_epochs = 2\n",
    "\n",
    "L2reg_total_perbatch = 1.0e-5\n",
    "print(datetime.datetime.now())\n",
    "UUtf, VVtf, loss_list = train_TF_DAE([],X,  hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "BBtf= UUtf.dot(VVtf)\n",
    "evaluate(BBtf)\n",
    "\n",
    "\n",
    "for _ in range(20):\n",
    "    print(datetime.datetime.now())\n",
    "    UUtf, VVtf, loss_list = train_TF_DAE([UUtf, VVtf],X, hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    BBtf= UUtf.dot(VVtf)\n",
    "    evaluate(BBtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "90"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=500\n",
    "dropout_prob=0.0\n",
    "L2reg_total = 100000.0\n",
    "lam_W = 0.0  # L2 per layer\n",
    "bsize = 4096  # batch size for training\n",
    "train_epochs = 1\n",
    "\n",
    "L2reg_total_perbatch = 1.0e-4\n",
    "print(datetime.datetime.now())\n",
    "UUtf, VVtf, loss_list = train_TF_DAE([],X,  hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "BBtf= UUtf.dot(VVtf)\n",
    "evaluate(BBtf)\n",
    "\n",
    "\n",
    "for _ in range(20):\n",
    "    print(datetime.datetime.now())\n",
    "    UUtf, VVtf, loss_list = train_TF_DAE([UUtf, VVtf],X, hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    BBtf= UUtf.dot(VVtf)\n",
    "    evaluate(BBtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "91"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=500\n",
    "dropout_prob=0.0\n",
    "L2reg_total = 100000.0\n",
    "lam_W = 0.0  # L2 per layer\n",
    "bsize = 4096  # batch size for training\n",
    "train_epochs = 1\n",
    "\n",
    "L2reg_total_perbatch = 3.0e-5\n",
    "print(datetime.datetime.now())\n",
    "UUtf, VVtf, loss_list = train_TF_DAE([],X,  hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "BBtf= UUtf.dot(VVtf)\n",
    "evaluate(BBtf)\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    print(datetime.datetime.now())\n",
    "    UUtf, VVtf, loss_list = train_TF_DAE([UUtf, VVtf],X, hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    BBtf= UUtf.dot(VVtf)\n",
    "    evaluate(BBtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "92"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "93"
    }
   },
   "outputs": [],
   "source": [
    "==== 1000 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "94"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=1000\n",
    "dropout_prob=0.5\n",
    "L2reg_total = 100000.0\n",
    "lam_W = 0.0  # L2 per layer\n",
    "bsize = 32000  # batch size for training\n",
    "train_epochs = 50\n",
    "L2reg_total_perbatch=0.0\n",
    "\n",
    "\n",
    "#for  L2reg_total_perbatch in [ 1.0e-6, 2.0e-6, 5.0e-6, 1.0e-5, 2.0e-5, 5.0e-5, 1.0e-4   ]:\n",
    "for  dropout_prob in [ 0.25, 0.3333, 0.5, 0.66667, 0.75, 0.9, 0.95, 0.99   ]:\n",
    "\n",
    "    print(\"============================================================ dropout_prob = {}\".format(dropout_prob  ))    \n",
    "    print(datetime.datetime.now())\n",
    "    UUtf, VVtf, loss_list = train_TF_DAE([],X,  hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "    print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "95"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "96"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x = tf.constant(np.array([1, 2, 3])[:,None] )\n",
    "    #y = tf.broadcast_to(x, [3, 2]) \n",
    "    aa = tf.constant(np.array([[10, 30],[40, 60],[70,90]]) )\n",
    "    bb = tf.multiply(x, aa) \n",
    "    #print(sess.run(y)) \n",
    "    print(sess.run(aa))\n",
    "    print(sess.run(bb))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "97"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "98"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "99"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "100"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "101"
    }
   },
   "outputs": [],
   "source": [
    "dden_dim=500\n",
    "dropout_prob=0.0\n",
    "L2reg_total = 100000.0\n",
    "lam_W = 0.0  # L2 per layer\n",
    "bsize = 4096  # batch size for training\n",
    "train_epochs = 5\n",
    "\n",
    "L2reg_total_perbatch = 0.00001*1.0/userCnt * L2reg_total\n",
    "print(datetime.datetime.now())\n",
    "UUtf, VVtf, loss_list = train_TF_DAE([],X,  hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "BBtf= UUtf.dot(VVtf)\n",
    "evaluate(BBtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "102"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "103"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "104"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "105"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "106"
    }
   },
   "outputs": [],
   "source": [
    "dden_dim=500\n",
    "dropout_prob=0.0\n",
    "L2reg_total = 100000.0\n",
    "lam_W = 0.0  # L2 per layer\n",
    "bsize = 4096  # batch size for training\n",
    "train_epochs = 5\n",
    "\n",
    "L2reg_total_perbatch = 0.003*1.0/userCnt * L2reg_total\n",
    "print(datetime.datetime.now())\n",
    "UUtf, VVtf, loss_list = train_TF_DAE([],X,  hidden_dim, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "BBtf= UUtf.dot(VVtf)\n",
    "evaluate(BBtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "107"
    }
   },
   "outputs": [],
   "source": [
    "22222222222222222222222222222222222222222222222222222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "108"
    }
   },
   "outputs": [],
   "source": [
    "def train_TF_analytic(weightsList, X,  hidden_dim, L2reg_xtx, L2reg_total, dropout_prob, lam_W, train_epochs, bsize = 256):\n",
    "   \n",
    "    \n",
    "    n_dim=X.shape[1]\n",
    "    y_dim=n_dim\n",
    "\n",
    "    #defhidden_dim, \n",
    "    #dropout_prob, \n",
    "    #lam_W = 0.01  # L2 per layer\n",
    "    #bsize = 256  # batch size for training\n",
    "    # no bias terms in model\n",
    "\n",
    "    tf.reset_default_graph()  # reset graph if run many times\n",
    "    ############## input X and target y\n",
    "    # None is saved as batch_size\n",
    "    input_ph = tf.placeholder(tf.float32, shape=(None, n_dim), name='input')\n",
    "    target_ph = tf.placeholder(tf.float32, shape=(None, y_dim), name='target')\n",
    "    ############ regularization lambda\n",
    "    lam = tf.placeholder(tf.float32, shape=(), name='lambda')\n",
    "    lam_total = tf.placeholder(tf.float32, shape=(), name='lambda_total')\n",
    "    lam_xtx = tf.placeholder(tf.float32, shape=(), name='lambda_xtx')\n",
    "    \n",
    "    ########### network structure \n",
    "    model_dims = [n_dim, hidden_dim, y_dim]\n",
    "    # weight for the neural nets, no bias terms\n",
    "    Ws = []\n",
    "    for l, (din, dout) in enumerate(zip(model_dims[:-1], model_dims[1:])):\n",
    "        print(\"Layer %d: input dimension=%d, output dimension=%d\" % (l, din, dout))\n",
    "        if len(weightsList)==0:\n",
    "            Ws.append(tf.get_variable(name=\"weight%d\" % l, shape=[din, dout], \n",
    "                                  initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        else:\n",
    "            Ws.append(tf.Variable(initial_value=weightsList[l], name=\"weight%d\" % l))          \n",
    "    # define network architecture\n",
    "    h1 = input_ph\n",
    "    h = tf.nn.dropout(h1, rate=dropout_prob)\n",
    "    for w in Ws[:-1]:\n",
    "        h = tf.matmul(h, w)   # linear model\n",
    "    y_hat = tf.matmul(h, Ws[-1])\n",
    "    ########### define regularization and loss\n",
    "    reg = l2_regularizer(lam)        \n",
    "    reg_var = apply_regularization(reg, Ws)\n",
    "\n",
    "    reg_total = l2_regularizer(lam_total)        \n",
    "    #dummy = tf.get_variable(name=\"dummy\", shape=[din, dout], \n",
    "    #                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    reg_var_total = apply_regularization(reg_total, [ tf.matmul(Ws[0],Ws[1]) ] )\n",
    "\n",
    "    reg_xtx = l2_regularizer(lam_xtx)        \n",
    "    reg_var_xtx = apply_regularization(reg_xtx, [ tf.matmul( tf.multiply(tf.constant(np.sqrt(XtXdiag)[:,None]) , Ws[0] ),  Ws[1]) ] )\n",
    "\n",
    "    mse = tf.reduce_mean( tf.square(tf.subtract(target_ph, y_hat)))\n",
    "\n",
    "    loss = mse +    reg_var_total + reg_var +reg_var_xtx\n",
    "    ############# define optimizer\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    ########### start training\n",
    "    N = X.shape[0]\n",
    "    idxlist = np.arange(N)\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        loss_list = list()\n",
    "        mse_list = list()\n",
    "\n",
    "        for epoch in range(train_epochs):\n",
    "            loss_epoch = list()\n",
    "            mse_epoch = list()\n",
    "\n",
    "            np.random.shuffle(idxlist)\n",
    "            for bnum, st_idx in enumerate(range(0, N, bsize)):\n",
    "                end_idx = min(st_idx + bsize, N)\n",
    "\n",
    "                inp = X[idxlist[st_idx:end_idx]]\n",
    "                if sparse.isspmatrix(inp):\n",
    "                    inp = inp.toarray()\n",
    "                    inp = inp.astype('float32')           \n",
    "            \n",
    "                \n",
    "                \n",
    "                tar = inp\n",
    "\n",
    "                feed_dict = {input_ph:inp, target_ph:tar, lam:lam_W, lam_total:L2reg_total, lam_xtx:L2reg_xtx}\n",
    "\n",
    "                l, m, _ = sess.run([loss, mse, train_op], feed_dict=feed_dict)\n",
    "                loss_epoch.append(l)\n",
    "                mse_epoch.append(m)\n",
    "\n",
    "            loss_list.append(np.mean(loss_epoch))\n",
    "            mse_list.append(np.mean(mse_epoch))\n",
    "\n",
    "\n",
    "            print(\"Epoch: %d, training_rmse=%.3f, training_loss=%.3f\" % (epoch, np.sqrt(mse_list[-1]), loss_list[-1]))\n",
    "            print(datetime.datetime.now())\n",
    "            \n",
    " \n",
    "            if epoch+1 in [1,5,10,20,30,40,50]:\n",
    "                print(\"--epoch {}\".format(epoch))\n",
    "                UU= sess.run(Ws[0])\n",
    "                VV = sess.run(Ws[1] ) \n",
    "                BBtf= UU.dot(VV)\n",
    "                evaluate(BBtf)\n",
    "\n",
    "            \n",
    "        UU= sess.run(Ws[0])\n",
    "        VV = sess.run(Ws[1] ) \n",
    "    return [UU,VV, loss_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "109"
    }
   },
   "outputs": [],
   "source": [
    "def train_TF_analytic2(weightsList, X,  hidden_dim, L2reg_xtx, L2reg_total, dropout_prob, lam_W, train_epochs, bsize = 256):\n",
    "   \n",
    "    \n",
    "    n_dim=X.shape[1]\n",
    "    y_dim=n_dim\n",
    "\n",
    "    #defhidden_dim, \n",
    "    #dropout_prob, \n",
    "    #lam_W = 0.01  # L2 per layer\n",
    "    #bsize = 256  # batch size for training\n",
    "    # no bias terms in model\n",
    "\n",
    "    tf.reset_default_graph()  # reset graph if run many times\n",
    "    ############## input X and target y\n",
    "    # None is saved as batch_size\n",
    "    input_ph = tf.placeholder(tf.float32, shape=(None, n_dim), name='input')\n",
    "    target_ph = tf.placeholder(tf.float32, shape=(None, y_dim), name='target')\n",
    "    ########### network structure \n",
    "    model_dims = [n_dim, hidden_dim, y_dim]\n",
    "    # weight for the neural nets, no bias terms\n",
    "    Ws = []\n",
    "    for l, (din, dout) in enumerate(zip(model_dims[:-1], model_dims[1:])):\n",
    "        print(\"Layer %d: input dimension=%d, output dimension=%d\" % (l, din, dout))\n",
    "        if len(weightsList)==0:\n",
    "            Ws.append(tf.get_variable(name=\"weight%d\" % l, shape=[din, dout], \n",
    "                                  initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        else:\n",
    "            Ws.append(tf.Variable(initial_value=weightsList[l], name=\"weight%d\" % l))          \n",
    "    # define network architecture\n",
    "    h1 = input_ph\n",
    "    h = tf.nn.dropout(h1, rate=dropout_prob)\n",
    "    for w in Ws[:-1]:\n",
    "        h = tf.matmul(h, w)   # linear model\n",
    "    y_hat = tf.matmul(h, Ws[-1])\n",
    "    ########### define regularization and loss\n",
    "    l2_layers = tf.multiply ( tf.constant(lam_W)   , tf.reduce_sum( tf.square(Ws[0]))+tf.reduce_sum(tf.square(Ws[1])) )\n",
    "    \n",
    "    l2_tot_1= tf.matmul( tf.multiply(tf.constant(np.sqrt(L2reg_total+ L2reg_xtx* XtXdiag)[:,None]) , Ws[0] ),  Ws[1])\n",
    "    l2_tot= tf.reduce_sum(tf.square (l2_tot_1 ))\n",
    "    \n",
    "    mse = tf.reduce_mean( tf.square(tf.subtract(target_ph, y_hat)))\n",
    "\n",
    "    loss = mse +l2_layers + l2_tot\n",
    "    ############# define optimizer\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    ########### start training\n",
    "    N = X.shape[0]\n",
    "    idxlist = np.arange(N)\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        loss_list = list()\n",
    "        mse_list = list()\n",
    "\n",
    "        for epoch in range(train_epochs):\n",
    "            loss_epoch = list()\n",
    "            mse_epoch = list()\n",
    "\n",
    "            np.random.shuffle(idxlist)\n",
    "            for bnum, st_idx in enumerate(range(0, N, bsize)):\n",
    "                end_idx = min(st_idx + bsize, N)\n",
    "\n",
    "                inp = X[idxlist[st_idx:end_idx]]\n",
    "                if sparse.isspmatrix(inp):\n",
    "                    inp = inp.toarray()\n",
    "                    inp = inp.astype('float32')           \n",
    "            \n",
    "                \n",
    "                \n",
    "                tar = inp\n",
    "\n",
    "                feed_dict = {input_ph:inp, target_ph:tar}\n",
    "\n",
    "                l, m, _ = sess.run([loss, mse, train_op], feed_dict=feed_dict)\n",
    "                loss_epoch.append(l)\n",
    "                mse_epoch.append(m)\n",
    "\n",
    "            loss_list.append(np.mean(loss_epoch))\n",
    "            mse_list.append(np.mean(mse_epoch))\n",
    "\n",
    "\n",
    "            print(\"Epoch: %d, training_rmse=%.3f, training_loss=%.3f\" % (epoch, np.sqrt(mse_list[-1]), loss_list[-1]))\n",
    "            print(datetime.datetime.now())\n",
    "            \n",
    " \n",
    "            if epoch+1 in [1,5,10,20,30,40,50]:\n",
    "                print(\"--epoch {}\".format(epoch))\n",
    "                UU= sess.run(Ws[0])\n",
    "                VV = sess.run(Ws[1] ) \n",
    "                BBtf= UU.dot(VV)\n",
    "                evaluate(BBtf)\n",
    "\n",
    "            \n",
    "        UU= sess.run(Ws[0])\n",
    "        VV = sess.run(Ws[1] ) \n",
    "    return [UU,VV, loss_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "110"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=1000\n",
    "#dropout_prob=0.0\n",
    "#L2reg_total = 0.0\n",
    "bsize = 32000  # batch size for training\n",
    "train_epochs = 50\n",
    "\n",
    "lam_W = 0.0  # L2 per layer\n",
    "L2reg_total_perbatch=0.0\n",
    "L2reg_xtx_perbatch=2e-10 *2.0\n",
    "\n",
    "#for  L2reg_total_perbatch in [ 1.0e-6, 2.0e-6, 5.0e-6, 1.0e-5, 2.0e-5, 5.0e-5, 1.0e-4   ]:\n",
    "for  dropout_prob in [ 0.0  ]:\n",
    "\n",
    "    print(\"============================================================ dropout_prob = {}\".format(dropout_prob  ))    \n",
    "    print(datetime.datetime.now())\n",
    "    UUtf, VVtf, loss_list = train_TF_analytic([],X,  hidden_dim, L2reg_xtx_perbatch, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "    print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "111"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=1000\n",
    "#dropout_prob=0.0\n",
    "#L2reg_total = 0.0\n",
    "bsize = 32000  # batch size for training\n",
    "train_epochs = 50\n",
    "\n",
    "lam_W = 0.0  # L2 per layer\n",
    "L2reg_total_perbatch=0.0\n",
    "L2reg_xtx_perbatch=2e-10 *2.0\n",
    "\n",
    "#for  L2reg_total_perbatch in [ 1.0e-6, 2.0e-6, 5.0e-6, 1.0e-5, 2.0e-5, 5.0e-5, 1.0e-4   ]:\n",
    "for  dropout_prob in [ 0.0  ]:\n",
    "\n",
    "    print(\"============================================================ dropout_prob = {}\".format(dropout_prob  ))    \n",
    "    print(datetime.datetime.now())\n",
    "    UUtf, VVtf, loss_list = train_TF_analytic2([],X,  hidden_dim, L2reg_xtx_perbatch, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "    print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "112"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "113"
    }
   },
   "source": [
    "# analytic b=0 solution in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "114"
    }
   },
   "outputs": [],
   "source": [
    "def train_TF_b0(weightsList, X,  hidden_dim, L2reg_xtx, L2reg_total, dropout_prob, lam_W, train_epochs, bsize = 256):\n",
    "   \n",
    "    \n",
    "    n_dim=X.shape[1]\n",
    "    y_dim=n_dim\n",
    "\n",
    "    #defhidden_dim, \n",
    "    #dropout_prob, \n",
    "    #lam_W = 0.01  # L2 per layer\n",
    "    #bsize = 256  # batch size for training\n",
    "    # no bias terms in model\n",
    "\n",
    "    tf.reset_default_graph()  # reset graph if run many times\n",
    "    ############## input X and target y\n",
    "    # None is saved as batch_size\n",
    "    input_ph = tf.placeholder(tf.float32, shape=(None, n_dim), name='input')\n",
    "    target_ph = tf.placeholder(tf.float32, shape=(None, y_dim), name='target')\n",
    "    ########### network structure \n",
    "    model_dims = [n_dim, hidden_dim, y_dim]\n",
    "    # weight for the neural nets, no bias terms\n",
    "    Ws = []\n",
    "    for l, (din, dout) in enumerate(zip(model_dims[:-1], model_dims[1:])):\n",
    "        print(\"Layer %d: input dimension=%d, output dimension=%d\" % (l, din, dout))\n",
    "        if len(weightsList)==0:\n",
    "            Ws.append(tf.get_variable(name=\"weight%d\" % l, shape=[din, dout], \n",
    "                                  initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        else:\n",
    "            Ws.append(tf.Variable(initial_value=weightsList[l], name=\"weight%d\" % l))          \n",
    "    # define network architecture\n",
    "    h1 = input_ph\n",
    "    h = tf.nn.dropout(h1, rate=dropout_prob)\n",
    "    for w in Ws[:-1]:\n",
    "        h = tf.matmul(h, w)   # linear model\n",
    "        \n",
    "    weightDiag= tf.matrix_diag_part( tf.matmul(Ws[0],Ws[1]))\n",
    "    y_hat = tf.matmul(h, Ws[-1]) - tf.multiply(input_ph, weightDiag)\n",
    "    ########### define regularization and loss\n",
    "    l2_layers = tf.multiply ( tf.constant(lam_W)   , tf.reduce_sum( tf.square(Ws[0]))+tf.reduce_sum(tf.square(Ws[1])) )\n",
    "    \n",
    "    l2_tot_1= tf.matmul( tf.multiply(tf.constant(np.sqrt(L2reg_total+ L2reg_xtx* XtXdiag)[:,None]) , Ws[0] ),  Ws[1])\n",
    "    l2_tot= tf.reduce_sum(tf.square (l2_tot_1 ))\n",
    "    \n",
    "    l2tot_diag_1= tf.multiply(tf.constant(np.sqrt(L2reg_total+ L2reg_xtx* XtXdiag)), weightDiag )\n",
    "    l2tot_diag = tf.reduce_sum(tf.square(l2tot_diag_1))\n",
    "    \n",
    "    mse = tf.reduce_mean( tf.square(tf.subtract(target_ph, y_hat)))\n",
    "\n",
    "    loss = mse +l2_layers + l2_tot - l2tot_diag\n",
    "    ############# define optimizer\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    ########### start training\n",
    "    N = X.shape[0]\n",
    "    idxlist = np.arange(N)\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        loss_list = list()\n",
    "        mse_list = list()\n",
    "\n",
    "        for epoch in range(train_epochs):\n",
    "            loss_epoch = list()\n",
    "            mse_epoch = list()\n",
    "\n",
    "            np.random.shuffle(idxlist)\n",
    "            for bnum, st_idx in enumerate(range(0, N, bsize)):\n",
    "                end_idx = min(st_idx + bsize, N)\n",
    "\n",
    "                inp = X[idxlist[st_idx:end_idx]]\n",
    "                if sparse.isspmatrix(inp):\n",
    "                    inp = inp.toarray()\n",
    "                    inp = inp.astype('float32')           \n",
    "            \n",
    "                \n",
    "                \n",
    "                tar = inp\n",
    "\n",
    "                feed_dict = {input_ph:inp, target_ph:tar}\n",
    "\n",
    "                l, m, _ = sess.run([loss, mse, train_op], feed_dict=feed_dict)\n",
    "                loss_epoch.append(l)\n",
    "                mse_epoch.append(m)\n",
    "\n",
    "            loss_list.append(np.mean(loss_epoch))\n",
    "            mse_list.append(np.mean(mse_epoch))\n",
    "\n",
    "\n",
    "            print(\"Epoch: %d, training_rmse=%.3f, training_loss=%.3f\" % (epoch, np.sqrt(mse_list[-1]), loss_list[-1]))\n",
    "            print(datetime.datetime.now())\n",
    "            \n",
    " \n",
    "            if epoch+1 in [1,5,10,20,30,40,50,60,70,80,90]:\n",
    "                print(\"--epoch {}\".format(epoch))\n",
    "                UU= sess.run(Ws[0])\n",
    "                VV = sess.run(Ws[1] ) \n",
    "                BBtf= UU.dot(VV)\n",
    "                evaluate(BBtf)\n",
    "\n",
    "            \n",
    "        UU= sess.run(Ws[0])\n",
    "        VV = sess.run(Ws[1] ) \n",
    "    return [UU,VV, loss_list]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "115"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=1000\n",
    "dropout_prob=0.0\n",
    "#L2reg_total = 0.0\n",
    "bsize = 32000  # batch size for training\n",
    "train_epochs = 70\n",
    "\n",
    "lam_W = 0.0  # L2 per layer\n",
    "L2reg_total_perbatch=0.0\n",
    "L2reg_xtx_perbatch=2e-10 * 0.5\n",
    "\n",
    "#for  L2reg_total_perbatch in [ 1.0e-6, 2.0e-6, 5.0e-6, 1.0e-5, 2.0e-5, 5.0e-5, 1.0e-4   ]:\n",
    "#for  dropout_prob in [ 0.0  ]:\n",
    "for L2reg_total_perbatch in [ 2e-5 / 100000.0*500.0 *100.0,  2e-5 / 100000.0*500.0 /100.0 ]:\n",
    "    print(\"============================================================ dropout_prob = {}\".format(dropout_prob  ))    \n",
    "    print(datetime.datetime.now())\n",
    "    UUtf, VVtf, loss_list = train_TF_b0([],X,  hidden_dim, L2reg_xtx_perbatch, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "    print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "116"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=1000\n",
    "dropout_prob=0.0\n",
    "#L2reg_total = 0.0\n",
    "bsize = 32000  # batch size for training\n",
    "train_epochs = 20\n",
    "\n",
    "lam_W = 0.0  # L2 per layer\n",
    "L2reg_total_perbatch=0.0\n",
    "L2reg_xtx_perbatch=2e-10 * 0.5\n",
    "\n",
    "#for  L2reg_total_perbatch in [ 1.0e-6, 2.0e-6, 5.0e-6, 1.0e-5, 2.0e-5, 5.0e-5, 1.0e-4   ]:\n",
    "#for  dropout_prob in [ 0.0  ]:\n",
    "for L2reg_total_perbatch in [ 0.0  ]:\n",
    "  for L2reg_xtx_perbatch in [ 1e-12 ]:\n",
    "    print(\"============================================================ dropout_prob = {}\".format(dropout_prob  ))    \n",
    "    print(datetime.datetime.now())\n",
    "    UUtf, VVtf, loss_list = train_TF_b0([UUtf, VVtf],X,  hidden_dim, L2reg_xtx_perbatch, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "    print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "117"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=1000\n",
    "dropout_prob=0.0\n",
    "#L2reg_total = 0.0\n",
    "bsize = 32000  # batch size for training\n",
    "train_epochs = 70\n",
    "\n",
    "lam_W = 0.0  # L2 per layer\n",
    "L2reg_total_perbatch=0.0\n",
    "L2reg_xtx_perbatch=2e-10 * 0.5\n",
    "\n",
    "#for  L2reg_total_perbatch in [ 1.0e-6, 2.0e-6, 5.0e-6, 1.0e-5, 2.0e-5, 5.0e-5, 1.0e-4   ]:\n",
    "#for  dropout_prob in [ 0.0  ]:\n",
    "for L2reg_total_perbatch in [ 0.0 ]:\n",
    "  for L2reg_xtx_perbatch in [ 1e-10]:\n",
    "    print(\"============================================================ dropout_prob = {}\".format(dropout_prob  ))    \n",
    "    print(datetime.datetime.now())\n",
    "    UUtf, VVtf, loss_list = train_TF_b0([],X,  hidden_dim, L2reg_xtx_perbatch, L2reg_total_perbatch, dropout_prob, lam_W, train_epochs, bsize )\n",
    "    print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "118"
    }
   },
   "outputs": [],
   "source": [
    "# effective catalog size via entropy ... Theil index\n",
    "\n",
    "qqqq= (XtXdiag / np.sum(XtXdiag)  )\n",
    "-qqqq.dot(np.log(qqqq))   # log is natural logsrithim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "119"
    }
   },
   "outputs": [],
   "source": [
    "np.exp(7.967947)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "120"
    }
   },
   "source": [
    "# AN EDLA with beta learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "121"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nterop": {
     "id": "122"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb prob_dropout= 0.05\n",
      "precomputing\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa dim=10\n",
      "2020-05-06 23:21:57.762949\n",
      "2020-05-06 23:21:57.771322\n",
      "================= iterCnt: 0\n",
      "2020-05-06 23:22:04.442526\n",
      "================= iterCnt: 1\n",
      "2020-05-06 23:22:13.603850\n",
      "================= iterCnt: 2\n",
      "2020-05-06 23:22:22.788427\n",
      "================= iterCnt: 3\n",
      "2020-05-06 23:22:31.949101\n",
      "================= iterCnt: 4\n",
      "2020-05-06 23:22:41.130425\n",
      "========================= eval:\n",
      "2020-05-06 23:22:42.288478\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.31949 (0.00203)\n",
      "Test Recall@20=0.28149 (0.00249)\n",
      "Test Recall@50=0.39849 (0.00284)\n",
      "2020-05-06 23:23:25.241326\n",
      "[ 10,   5,   0.050,   500,   0.319,   0.281,  0.398 ]\n",
      "2020-05-06 23:23:25.255187\n",
      "================= iterCnt: 5\n",
      "2020-05-06 23:23:34.702010\n",
      "================= iterCnt: 6\n",
      "2020-05-06 23:23:44.202635\n",
      "================= iterCnt: 7\n",
      "2020-05-06 23:23:53.681218\n",
      "================= iterCnt: 8\n",
      "2020-05-06 23:24:03.156729\n",
      "================= iterCnt: 9\n",
      "2020-05-06 23:24:12.675880\n",
      "========================= eval:\n",
      "2020-05-06 23:24:13.859675\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.32355 (0.00204)\n",
      "Test Recall@20=0.28783 (0.00252)\n",
      "Test Recall@50=0.40664 (0.00287)\n",
      "2020-05-06 23:24:56.648511\n",
      "[ 10,   5,   0.050,   500,   0.324,   0.288,  0.407 ]\n",
      "2020-05-06 23:24:56.662681\n",
      "================= iterCnt: 10\n",
      "2020-05-06 23:25:06.196840\n",
      "================= iterCnt: 11\n",
      "2020-05-06 23:25:15.750814\n",
      "================= iterCnt: 12\n",
      "2020-05-06 23:25:25.229475\n",
      "================= iterCnt: 13\n",
      "2020-05-06 23:25:34.720683\n",
      "================= iterCnt: 14\n",
      "2020-05-06 23:25:44.226187\n",
      "================= iterCnt: 15\n",
      "2020-05-06 23:25:53.712843\n",
      "================= iterCnt: 16\n",
      "2020-05-06 23:26:02.891718\n",
      "================= iterCnt: 17\n",
      "2020-05-06 23:26:12.123276\n",
      "================= iterCnt: 18\n",
      "2020-05-06 23:26:21.342391\n",
      "================= iterCnt: 19\n",
      "2020-05-06 23:26:30.569795\n",
      "========================= eval:\n",
      "2020-05-06 23:26:31.725100\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.32437 (0.00204)\n",
      "Test Recall@20=0.28967 (0.00253)\n",
      "Test Recall@50=0.40818 (0.00288)\n",
      "2020-05-06 23:27:15.479837\n",
      "[ 10,   5,   0.050,   500,   0.324,   0.290,  0.408 ]\n",
      "2020-05-06 23:27:15.492971\n",
      "================= iterCnt: 20\n",
      "2020-05-06 23:27:24.585327\n",
      "================= iterCnt: 21\n",
      "2020-05-06 23:27:33.725892\n",
      "================= iterCnt: 22\n",
      "2020-05-06 23:27:42.875024\n",
      "================= iterCnt: 23\n",
      "2020-05-06 23:27:52.014432\n",
      "================= iterCnt: 24\n",
      "2020-05-06 23:28:01.132296\n",
      "================= iterCnt: 25\n",
      "2020-05-06 23:28:10.303204\n",
      "================= iterCnt: 26\n",
      "2020-05-06 23:28:19.429065\n",
      "================= iterCnt: 27\n",
      "2020-05-06 23:28:28.581498\n",
      "================= iterCnt: 28\n",
      "2020-05-06 23:28:37.713184\n",
      "================= iterCnt: 29\n",
      "2020-05-06 23:28:46.874183\n",
      "========================= eval:\n",
      "2020-05-06 23:28:48.016464\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.32464 (0.00204)\n",
      "Test Recall@20=0.28925 (0.00253)\n",
      "Test Recall@50=0.40816 (0.00288)\n",
      "2020-05-06 23:29:30.521281\n",
      "[ 10,   5,   0.050,   500,   0.325,   0.289,  0.408 ]\n",
      "2020-05-06 23:29:30.534937\n",
      "================= iterCnt: 30\n",
      "2020-05-06 23:29:39.694237\n",
      "================= iterCnt: 31\n",
      "2020-05-06 23:29:48.927273\n",
      "================= iterCnt: 32\n",
      "2020-05-06 23:29:58.125765\n",
      "================= iterCnt: 33\n",
      "2020-05-06 23:30:07.251267\n",
      "================= iterCnt: 34\n",
      "2020-05-06 23:30:16.367228\n",
      "================= iterCnt: 35\n",
      "2020-05-06 23:30:25.542362\n",
      "================= iterCnt: 36\n",
      "2020-05-06 23:30:34.929340\n",
      "================= iterCnt: 37\n",
      "2020-05-06 23:30:44.396434\n",
      "================= iterCnt: 38\n",
      "2020-05-06 23:30:53.946301\n",
      "================= iterCnt: 39\n",
      "2020-05-06 23:31:03.432223\n",
      "========================= eval:\n",
      "2020-05-06 23:31:04.603316\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.32459 (0.00204)\n",
      "Test Recall@20=0.28920 (0.00253)\n",
      "Test Recall@50=0.40825 (0.00288)\n",
      "2020-05-06 23:31:47.321654\n",
      "[ 10,   5,   0.050,   500,   0.325,   0.289,  0.408 ]\n",
      "2020-05-06 23:31:47.335457\n",
      "================= iterCnt: 40\n",
      "2020-05-06 23:31:56.755169\n",
      "================= iterCnt: 41\n",
      "2020-05-06 23:32:06.773965\n",
      "================= iterCnt: 42\n",
      "2020-05-06 23:32:16.526755\n",
      "================= iterCnt: 43\n",
      "2020-05-06 23:32:26.248556\n",
      "================= iterCnt: 44\n",
      "2020-05-06 23:32:35.870203\n",
      "================= iterCnt: 45\n",
      "2020-05-06 23:32:45.492358\n",
      "================= iterCnt: 46\n",
      "2020-05-06 23:32:55.092805\n",
      "================= iterCnt: 47\n",
      "2020-05-06 23:33:04.701091\n",
      "================= iterCnt: 48\n",
      "2020-05-06 23:33:14.371269\n",
      "================= iterCnt: 49\n",
      "2020-05-06 23:33:24.337711\n",
      "========================= eval:\n",
      "2020-05-06 23:33:25.629907\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.32461 (0.00204)\n",
      "Test Recall@20=0.28873 (0.00253)\n",
      "Test Recall@50=0.40834 (0.00287)\n",
      "2020-05-06 23:34:08.834373\n",
      "[ 10,   5,   0.050,   500,   0.325,   0.289,  0.408 ]\n",
      "2020-05-06 23:34:08.848793\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa dim=20\n",
      "2020-05-06 23:34:08.849342\n",
      "2020-05-06 23:34:08.866302\n",
      "================= iterCnt: 0\n",
      "2020-05-06 23:34:15.882457\n",
      "================= iterCnt: 1\n",
      "2020-05-06 23:34:25.534135\n",
      "================= iterCnt: 2\n",
      "2020-05-06 23:34:35.226779\n",
      "================= iterCnt: 3\n",
      "2020-05-06 23:34:44.901079\n",
      "================= iterCnt: 4\n",
      "2020-05-06 23:34:54.482291\n",
      "========================= eval:\n",
      "2020-05-06 23:34:55.650692\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.35133 (0.00209)\n",
      "Test Recall@20=0.31974 (0.00262)\n",
      "Test Recall@50=0.43727 (0.00288)\n",
      "2020-05-06 23:35:39.506948\n",
      "[ 20,   5,   0.050,   500,   0.351,   0.320,  0.437 ]\n",
      "2020-05-06 23:35:39.520658\n",
      "================= iterCnt: 5\n",
      "2020-05-06 23:35:49.386370\n",
      "================= iterCnt: 6\n",
      "2020-05-06 23:35:59.270863\n",
      "================= iterCnt: 7\n",
      "2020-05-06 23:36:09.041535\n",
      "================= iterCnt: 8\n",
      "2020-05-06 23:36:18.675284\n",
      "================= iterCnt: 9\n",
      "2020-05-06 23:36:28.254563\n",
      "========================= eval:\n",
      "2020-05-06 23:36:29.426126\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.35422 (0.00210)\n",
      "Test Recall@20=0.32165 (0.00263)\n",
      "Test Recall@50=0.44103 (0.00288)\n",
      "2020-05-06 23:37:12.293264\n",
      "[ 20,   5,   0.050,   500,   0.354,   0.322,  0.441 ]\n",
      "2020-05-06 23:37:12.307420\n",
      "================= iterCnt: 10\n",
      "2020-05-06 23:37:21.908698\n",
      "================= iterCnt: 11\n",
      "2020-05-06 23:37:31.435237\n",
      "================= iterCnt: 12\n",
      "2020-05-06 23:37:40.893434\n",
      "================= iterCnt: 13\n",
      "2020-05-06 23:37:50.357950\n",
      "================= iterCnt: 14\n",
      "2020-05-06 23:38:00.219841\n",
      "================= iterCnt: 15\n",
      "2020-05-06 23:38:10.236857\n",
      "================= iterCnt: 16\n",
      "2020-05-06 23:38:19.950198\n",
      "================= iterCnt: 17\n",
      "2020-05-06 23:38:29.561492\n",
      "================= iterCnt: 18\n",
      "2020-05-06 23:38:39.140472\n",
      "================= iterCnt: 19\n",
      "2020-05-06 23:38:48.438557\n",
      "========================= eval:\n",
      "2020-05-06 23:38:49.591679\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.35603 (0.00210)\n",
      "Test Recall@20=0.32243 (0.00263)\n",
      "Test Recall@50=0.44381 (0.00288)\n",
      "2020-05-06 23:39:32.875192\n",
      "[ 20,   5,   0.050,   500,   0.356,   0.322,  0.444 ]\n",
      "2020-05-06 23:39:32.888832\n",
      "================= iterCnt: 20\n",
      "2020-05-06 23:39:42.113148\n",
      "================= iterCnt: 21\n",
      "2020-05-06 23:39:51.377590\n",
      "================= iterCnt: 22\n",
      "2020-05-06 23:40:00.699414\n",
      "================= iterCnt: 23\n",
      "2020-05-06 23:40:10.005562\n",
      "================= iterCnt: 24\n",
      "2020-05-06 23:40:19.257239\n",
      "================= iterCnt: 25\n",
      "2020-05-06 23:40:28.532440\n",
      "================= iterCnt: 26\n",
      "2020-05-06 23:40:37.814675\n",
      "================= iterCnt: 27\n",
      "2020-05-06 23:40:47.041672\n",
      "================= iterCnt: 28\n",
      "2020-05-06 23:40:56.484905\n",
      "================= iterCnt: 29\n",
      "2020-05-06 23:41:05.749115\n",
      "========================= eval:\n",
      "2020-05-06 23:41:06.903348\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.35624 (0.00209)\n",
      "Test Recall@20=0.32231 (0.00263)\n",
      "Test Recall@50=0.44483 (0.00288)\n",
      "2020-05-06 23:41:49.692949\n",
      "[ 20,   5,   0.050,   500,   0.356,   0.322,  0.445 ]\n",
      "2020-05-06 23:41:49.706641\n",
      "================= iterCnt: 30\n",
      "2020-05-06 23:41:58.986824\n",
      "================= iterCnt: 31\n",
      "2020-05-06 23:42:08.214391\n",
      "================= iterCnt: 32\n",
      "2020-05-06 23:42:17.462390\n",
      "================= iterCnt: 33\n",
      "2020-05-06 23:42:26.835459\n",
      "================= iterCnt: 34\n",
      "2020-05-06 23:42:36.048531\n",
      "================= iterCnt: 35\n",
      "2020-05-06 23:42:45.313211\n",
      "================= iterCnt: 36\n",
      "2020-05-06 23:42:54.545364\n",
      "================= iterCnt: 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-06 23:43:03.775228\n",
      "================= iterCnt: 38\n",
      "2020-05-06 23:43:13.039542\n",
      "================= iterCnt: 39\n",
      "2020-05-06 23:43:22.285584\n",
      "========================= eval:\n",
      "2020-05-06 23:43:23.437820\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.35612 (0.00209)\n",
      "Test Recall@20=0.32270 (0.00263)\n",
      "Test Recall@50=0.44654 (0.00289)\n",
      "2020-05-06 23:44:06.518930\n",
      "[ 20,   5,   0.050,   500,   0.356,   0.323,  0.447 ]\n",
      "2020-05-06 23:44:06.532644\n",
      "================= iterCnt: 40\n",
      "2020-05-06 23:44:15.760172\n",
      "================= iterCnt: 41\n",
      "2020-05-06 23:44:24.957397\n",
      "================= iterCnt: 42\n",
      "2020-05-06 23:44:34.106686\n",
      "================= iterCnt: 43\n",
      "2020-05-06 23:44:43.274504\n",
      "================= iterCnt: 44\n",
      "2020-05-06 23:44:52.425809\n",
      "================= iterCnt: 45\n",
      "2020-05-06 23:45:01.692205\n",
      "================= iterCnt: 46\n",
      "2020-05-06 23:45:10.859619\n",
      "================= iterCnt: 47\n",
      "2020-05-06 23:45:20.018290\n",
      "================= iterCnt: 48\n",
      "2020-05-06 23:45:29.167495\n",
      "================= iterCnt: 49\n",
      "2020-05-06 23:45:38.354379\n",
      "========================= eval:\n",
      "2020-05-06 23:45:39.487428\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.35632 (0.00208)\n",
      "Test Recall@20=0.32380 (0.00263)\n",
      "Test Recall@50=0.44684 (0.00288)\n",
      "2020-05-06 23:46:28.494414\n",
      "[ 20,   5,   0.050,   500,   0.356,   0.324,  0.447 ]\n",
      "2020-05-06 23:46:28.507792\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa dim=100\n",
      "2020-05-06 23:46:28.508288\n",
      "2020-05-06 23:46:28.591096\n",
      "================= iterCnt: 0\n",
      "2020-05-06 23:46:38.159013\n",
      "================= iterCnt: 1\n",
      "2020-05-06 23:46:49.977160\n",
      "================= iterCnt: 2\n",
      "2020-05-06 23:47:01.349121\n",
      "================= iterCnt: 3\n",
      "2020-05-06 23:47:12.712661\n",
      "================= iterCnt: 4\n",
      "2020-05-06 23:47:23.957383\n",
      "========================= eval:\n",
      "2020-05-06 23:47:25.293017\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.38356 (0.00212)\n",
      "Test Recall@20=0.35248 (0.00263)\n",
      "Test Recall@50=0.47543 (0.00284)\n",
      "2020-05-06 23:48:09.165854\n",
      "[ 100,   5,   0.050,   500,   0.384,   0.352,  0.475 ]\n",
      "2020-05-06 23:48:09.179126\n",
      "================= iterCnt: 5\n",
      "2020-05-06 23:48:20.161005\n",
      "================= iterCnt: 6\n",
      "2020-05-06 23:48:31.141575\n",
      "================= iterCnt: 7\n",
      "2020-05-06 23:48:42.437533\n",
      "================= iterCnt: 8\n",
      "2020-05-06 23:48:53.542133\n",
      "================= iterCnt: 9\n",
      "2020-05-06 23:49:04.702139\n",
      "========================= eval:\n",
      "2020-05-06 23:49:05.993905\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.39831 (0.00215)\n",
      "Test Recall@20=0.36986 (0.00269)\n",
      "Test Recall@50=0.49171 (0.00286)\n",
      "2020-05-06 23:49:49.269903\n",
      "[ 100,   5,   0.050,   500,   0.398,   0.370,  0.492 ]\n",
      "2020-05-06 23:49:49.283160\n",
      "================= iterCnt: 10\n",
      "2020-05-06 23:50:00.231880\n",
      "================= iterCnt: 11\n",
      "2020-05-06 23:50:11.232936\n",
      "================= iterCnt: 12\n",
      "2020-05-06 23:50:22.184368\n",
      "================= iterCnt: 13\n",
      "2020-05-06 23:50:33.157661\n",
      "================= iterCnt: 14\n",
      "2020-05-06 23:50:44.400680\n",
      "================= iterCnt: 15\n",
      "2020-05-06 23:50:55.356308\n",
      "================= iterCnt: 16\n",
      "2020-05-06 23:51:06.774465\n",
      "================= iterCnt: 17\n",
      "2020-05-06 23:51:18.070810\n",
      "================= iterCnt: 18\n",
      "2020-05-06 23:51:29.382003\n",
      "================= iterCnt: 19\n",
      "2020-05-06 23:51:40.830812\n",
      "========================= eval:\n",
      "2020-05-06 23:51:42.167236\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.40504 (0.00216)\n",
      "Test Recall@20=0.37498 (0.00268)\n",
      "Test Recall@50=0.50065 (0.00286)\n",
      "2020-05-06 23:52:25.807478\n",
      "[ 100,   5,   0.050,   500,   0.405,   0.375,  0.501 ]\n",
      "2020-05-06 23:52:25.821925\n",
      "================= iterCnt: 20\n",
      "2020-05-06 23:52:37.317251\n",
      "================= iterCnt: 21\n",
      "2020-05-06 23:52:48.652907\n",
      "================= iterCnt: 22\n",
      "2020-05-06 23:53:00.231373\n",
      "================= iterCnt: 23\n",
      "2020-05-06 23:53:11.762874\n",
      "================= iterCnt: 24\n",
      "2020-05-06 23:53:23.129234\n",
      "================= iterCnt: 25\n",
      "2020-05-06 23:53:34.413365\n",
      "================= iterCnt: 26\n",
      "2020-05-06 23:53:45.838060\n",
      "================= iterCnt: 27\n",
      "2020-05-06 23:53:57.101214\n",
      "================= iterCnt: 28\n",
      "2020-05-06 23:54:08.358543\n",
      "================= iterCnt: 29\n",
      "2020-05-06 23:54:19.659646\n",
      "========================= eval:\n",
      "2020-05-06 23:54:20.994491\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.40610 (0.00215)\n",
      "Test Recall@20=0.37503 (0.00268)\n",
      "Test Recall@50=0.50279 (0.00286)\n",
      "2020-05-06 23:55:04.159104\n",
      "[ 100,   5,   0.050,   500,   0.406,   0.375,  0.503 ]\n",
      "2020-05-06 23:55:04.172921\n",
      "================= iterCnt: 30\n",
      "2020-05-06 23:55:15.616009\n",
      "================= iterCnt: 31\n",
      "2020-05-06 23:55:26.913572\n",
      "================= iterCnt: 32\n",
      "2020-05-06 23:55:38.196653\n",
      "================= iterCnt: 33\n",
      "2020-05-06 23:55:49.656678\n",
      "================= iterCnt: 34\n",
      "2020-05-06 23:56:00.990710\n",
      "================= iterCnt: 35\n",
      "2020-05-06 23:56:12.341797\n",
      "================= iterCnt: 36\n",
      "2020-05-06 23:56:23.577443\n",
      "================= iterCnt: 37\n",
      "2020-05-06 23:56:34.842163\n",
      "================= iterCnt: 38\n",
      "2020-05-06 23:56:46.184537\n",
      "================= iterCnt: 39\n",
      "2020-05-06 23:56:57.444524\n",
      "========================= eval:\n",
      "2020-05-06 23:56:58.778482\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.40592 (0.00214)\n",
      "Test Recall@20=0.37498 (0.00268)\n",
      "Test Recall@50=0.50270 (0.00287)\n",
      "2020-05-06 23:57:42.020690\n",
      "[ 100,   5,   0.050,   500,   0.406,   0.375,  0.503 ]\n",
      "2020-05-06 23:57:42.034143\n",
      "================= iterCnt: 40\n",
      "2020-05-06 23:57:52.990031\n",
      "================= iterCnt: 41\n",
      "2020-05-06 23:58:04.062735\n",
      "================= iterCnt: 42\n",
      "2020-05-06 23:58:15.325734\n",
      "================= iterCnt: 43\n",
      "2020-05-06 23:58:26.424267\n",
      "================= iterCnt: 44\n",
      "2020-05-06 23:58:37.413276\n",
      "================= iterCnt: 45\n",
      "2020-05-06 23:58:48.471934\n",
      "================= iterCnt: 46\n",
      "2020-05-06 23:58:59.414517\n",
      "================= iterCnt: 47\n",
      "2020-05-06 23:59:10.676257\n",
      "================= iterCnt: 48\n",
      "2020-05-06 23:59:21.947009\n",
      "================= iterCnt: 49\n",
      "2020-05-06 23:59:33.449893\n",
      "========================= eval:\n",
      "2020-05-06 23:59:34.799115\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.40506 (0.00214)\n",
      "Test Recall@20=0.37401 (0.00267)\n",
      "Test Recall@50=0.50262 (0.00286)\n",
      "2020-05-07 00:00:18.103018\n",
      "[ 100,   5,   0.050,   500,   0.405,   0.374,  0.503 ]\n",
      "2020-05-07 00:00:18.116536\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa dim=200\n",
      "2020-05-07 00:00:18.117091\n",
      "2020-05-07 00:00:18.282989\n",
      "================= iterCnt: 0\n",
      "2020-05-07 00:00:28.149532\n",
      "================= iterCnt: 1\n",
      "2020-05-07 00:00:41.792020\n",
      "================= iterCnt: 2\n",
      "2020-05-07 00:00:55.681347\n",
      "================= iterCnt: 3\n",
      "2020-05-07 00:01:09.328350\n",
      "================= iterCnt: 4\n",
      "2020-05-07 00:01:22.939307\n",
      "========================= eval:\n",
      "2020-05-07 00:01:24.523166\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.38576 (0.00210)\n",
      "Test Recall@20=0.35828 (0.00263)\n",
      "Test Recall@50=0.48131 (0.00282)\n",
      "2020-05-07 00:02:08.029512\n",
      "[ 200,   5,   0.050,   500,   0.386,   0.358,  0.481 ]\n",
      "2020-05-07 00:02:08.043557\n",
      "================= iterCnt: 5\n",
      "2020-05-07 00:02:21.932186\n",
      "================= iterCnt: 6\n",
      "2020-05-07 00:02:35.931454\n",
      "================= iterCnt: 7\n",
      "2020-05-07 00:02:49.895647\n",
      "================= iterCnt: 8\n",
      "2020-05-07 00:03:03.838304\n",
      "================= iterCnt: 9\n",
      "2020-05-07 00:03:17.750981\n",
      "========================= eval:\n",
      "2020-05-07 00:03:19.364745\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.40872 (0.00216)\n",
      "Test Recall@20=0.38033 (0.00268)\n",
      "Test Recall@50=0.50513 (0.00284)\n",
      "2020-05-07 00:04:02.897769\n",
      "[ 200,   5,   0.050,   500,   0.409,   0.380,  0.505 ]\n",
      "2020-05-07 00:04:02.911012\n",
      "================= iterCnt: 10\n",
      "2020-05-07 00:04:16.356784\n",
      "================= iterCnt: 11\n",
      "2020-05-07 00:04:30.072581\n",
      "================= iterCnt: 12\n",
      "2020-05-07 00:04:43.549090\n",
      "================= iterCnt: 13\n",
      "2020-05-07 00:04:57.052280\n",
      "================= iterCnt: 14\n",
      "2020-05-07 00:05:10.937301\n",
      "================= iterCnt: 15\n",
      "2020-05-07 00:05:24.865121\n",
      "================= iterCnt: 16\n",
      "2020-05-07 00:05:38.780460\n",
      "================= iterCnt: 17\n",
      "2020-05-07 00:05:52.782112\n",
      "================= iterCnt: 18\n",
      "2020-05-07 00:06:06.680142\n",
      "================= iterCnt: 19\n",
      "2020-05-07 00:06:20.631407\n",
      "========================= eval:\n",
      "2020-05-07 00:06:22.258238\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.41640 (0.00216)\n",
      "Test Recall@20=0.38777 (0.00270)\n",
      "Test Recall@50=0.51505 (0.00285)\n",
      "2020-05-07 00:07:05.917588\n",
      "[ 200,   5,   0.050,   500,   0.416,   0.388,  0.515 ]\n",
      "2020-05-07 00:07:05.931243\n",
      "================= iterCnt: 20\n",
      "2020-05-07 00:07:19.705575\n",
      "================= iterCnt: 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-07 00:07:33.564845\n",
      "================= iterCnt: 22\n",
      "2020-05-07 00:07:47.407025\n",
      "================= iterCnt: 23\n",
      "2020-05-07 00:08:01.729125\n",
      "================= iterCnt: 24\n",
      "2020-05-07 00:08:15.664971\n",
      "================= iterCnt: 25\n",
      "2020-05-07 00:08:29.386852\n",
      "================= iterCnt: 26\n",
      "2020-05-07 00:08:42.892696\n",
      "================= iterCnt: 27\n",
      "2020-05-07 00:08:56.403838\n",
      "================= iterCnt: 28\n",
      "2020-05-07 00:09:10.093404\n",
      "================= iterCnt: 29\n",
      "2020-05-07 00:09:23.853952\n",
      "========================= eval:\n",
      "2020-05-07 00:09:25.444730\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.41633 (0.00215)\n",
      "Test Recall@20=0.38655 (0.00269)\n",
      "Test Recall@50=0.51517 (0.00285)\n",
      "2020-05-07 00:10:08.725816\n",
      "[ 200,   5,   0.050,   500,   0.416,   0.387,  0.515 ]\n",
      "2020-05-07 00:10:08.739752\n",
      "================= iterCnt: 30\n",
      "2020-05-07 00:10:22.250505\n",
      "================= iterCnt: 31\n",
      "2020-05-07 00:10:35.758211\n",
      "================= iterCnt: 32\n",
      "2020-05-07 00:10:49.667541\n",
      "================= iterCnt: 33\n",
      "2020-05-07 00:11:03.837469\n",
      "================= iterCnt: 34\n",
      "2020-05-07 00:11:17.708954\n",
      "================= iterCnt: 35\n",
      "2020-05-07 00:11:31.626078\n",
      "================= iterCnt: 36\n",
      "2020-05-07 00:11:45.526558\n",
      "================= iterCnt: 37\n",
      "2020-05-07 00:11:59.402849\n",
      "================= iterCnt: 38\n",
      "2020-05-07 00:12:13.213339\n",
      "================= iterCnt: 39\n",
      "2020-05-07 00:12:27.073897\n",
      "========================= eval:\n",
      "2020-05-07 00:12:28.643545\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.41528 (0.00214)\n",
      "Test Recall@20=0.38299 (0.00268)\n",
      "Test Recall@50=0.51374 (0.00285)\n",
      "2020-05-07 00:13:12.121577\n",
      "[ 200,   5,   0.050,   500,   0.415,   0.383,  0.514 ]\n",
      "2020-05-07 00:13:12.134718\n",
      "================= iterCnt: 40\n",
      "2020-05-07 00:13:25.636966\n",
      "================= iterCnt: 41\n",
      "2020-05-07 00:13:39.126222\n",
      "================= iterCnt: 42\n",
      "2020-05-07 00:13:52.659706\n",
      "================= iterCnt: 43\n",
      "2020-05-07 00:14:06.472282\n",
      "================= iterCnt: 44\n",
      "2020-05-07 00:14:20.281448\n",
      "================= iterCnt: 45\n",
      "2020-05-07 00:14:34.099078\n",
      "================= iterCnt: 46\n",
      "2020-05-07 00:14:47.583793\n",
      "================= iterCnt: 47\n",
      "2020-05-07 00:15:01.192926\n",
      "================= iterCnt: 48\n",
      "2020-05-07 00:15:14.657221\n",
      "================= iterCnt: 49\n",
      "2020-05-07 00:15:28.445672\n",
      "========================= eval:\n",
      "2020-05-07 00:15:30.061450\n",
      "0 ... 5000\n",
      "5000 ... 10000\n",
      "Test NDCG@100=0.41507 (0.00214)\n",
      "Test Recall@20=0.38206 (0.00268)\n",
      "Test Recall@50=0.51278 (0.00284)\n",
      "2020-05-07 00:16:13.282981\n",
      "[ 200,   5,   0.050,   500,   0.415,   0.382,  0.513 ]\n",
      "2020-05-07 00:16:13.296495\n"
     ]
    }
   ],
   "source": [
    "# EDLAe with beta learning\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"precomputing\")\n",
    "#precompute\n",
    "#ii_diag=np.diag_indices(XtX.shape[0])\n",
    "#XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag \n",
    "#PP=np.linalg.inv(XtX)\n",
    "\n",
    "for rhoadd in [500 ]:\n",
    "  for prob_dropout in [  0.05]:\n",
    "    boost= prob_dropout/(1.0-prob_dropout)\n",
    "    print(\"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb prob_dropout= {}\".format(prob_dropout))\n",
    "    for L2reg in [  5.0 ]:\n",
    "      #precompute\n",
    "      print(\"precomputing\")\n",
    "      rhoVec = boost*XtXdiag  + L2reg +rhoadd\n",
    "      ii_diag=np.diag_indices(XtX.shape[0])\n",
    "      XtX[ii_diag]= XtXdiag +boost*XtXdiag  + L2reg+ rhoVec\n",
    "      PP=np.linalg.inv(XtX)\n",
    "      for dim in  [ 10, 20, 100, 200]:\n",
    "        print(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa dim={}\".format(dim))\n",
    "\n",
    "        \n",
    "        \n",
    "        betaVec= np.zeros(XtX.shape[0])\n",
    "        etaVec= np.zeros(XtX.shape[0])\n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "        EE=np.random.randn(XtX.shape[0], dim) * 0.0001 #* sparsityMask\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "\n",
    "        for itercnt in range(50):\n",
    "            print(\"================= iterCnt: {}\".format(itercnt))\n",
    "            #print(\"  --- train FF\")\n",
    "            #print(datetime.datetime.now())\n",
    "            #### update FFt\n",
    "            ii_diag=np.diag_indices(XtX.shape[0])\n",
    "            XtX[ii_diag]= XtXdiag +boost*XtXdiag  + L2reg+ rhoVec\n",
    "            HH=EE.T.dot(XtX).dot(EE)\n",
    "            #print(\"          10\")\n",
    "            HH= np.linalg.inv(HH)  .dot(EE.T)\n",
    "            #print(\"          11\")\n",
    "            XtX[ii_diag]= XtXdiag\n",
    "            GG= XtX *(1.0+betaVec)\n",
    "            GG[ii_diag]+= rhoVec*  (betaVec-etaVec)\n",
    "            FFt= HH.dot(GG)\n",
    "            #######update EE\n",
    "            #print(\"          1\")\n",
    "            HH= FFt.dot(FFt.T)\n",
    "            HH=np.linalg.inv(HH)\n",
    "            HH=FFt.T.dot(HH)\n",
    "            #print(\"          3\")\n",
    "            XtX[ii_diag]= XtXdiag\n",
    "            GG= XtX *(1.0+betaVec)\n",
    "            GG[ii_diag]+= rhoVec*  (betaVec-etaVec)\n",
    "            EE= GG.dot(HH)\n",
    "            del GG\n",
    "            EE=PP.dot(EE)\n",
    "            ########## update betaVec\n",
    "            EEFFdiag  = np.diag(EE.dot(FFt))\n",
    "            if itercnt>0:\n",
    "                XtX[ii_diag]= XtXdiag\n",
    "                HH=np.diag(XtX.dot(EE).dot(FFt)) -XtXdiag +rhoVec* (etaVec+ EEFFdiag   )\n",
    "                GG=XtXdiag-boost*XtXdiag  - L2reg +rhoVec\n",
    "                betaVec=HH/GG\n",
    "                del GG\n",
    "                betaVec=np.maximum(betaVec, 0.0) # non-neg values !\n",
    "            if (itercnt+1) in [1000]: #[1,5,10,20,30,40,50,60,70,80,90,100]:\n",
    "                #plt.plot(betaTF, betaVec, 'x')\n",
    "                #plt.show()\n",
    "                plt.hist(EEFFdiag, bins=100, alpha=0.8,  density=True,histtype='step', color='black', linewidth=2.0,  label='calibrated')\n",
    "                plt.show()\n",
    "                print(\"avg diag value: {}\".format(np.mean(EEFFdiag)))\n",
    "                plt.plot(EEFFdiag, betaVec, 'x')\n",
    "                plt.show()\n",
    "                \n",
    "                    \n",
    "            if (itercnt+1) in [10000]: #[1,5,10,20,30,40,50,60,70,80,90,100]:\n",
    "                EEnn= (1.0 /np.sqrt(np.sum(EE*EE,1)))[:,None]  *EE\n",
    "                FFtnn= FFt / np.sqrt(np.sum(FFt*FFt,0))\n",
    "                cosineEDLAE=np.diag(EEnn.dot(FFtnn))\n",
    "                del EEnn\n",
    "                del FFtnn\n",
    "                plt.hist(cosineEDLAE, bins=100, alpha=0.8,  density=True,histtype='step', color='black', linewidth=2.0,  label='calibrated')\n",
    "                plt.show()\n",
    "\n",
    "            \n",
    "            ########## update etaVec\n",
    "            etaVec+= EEFFdiag-betaVec\n",
    "            \n",
    "            print(datetime.datetime.now())\n",
    "            ###### eval\n",
    "            if (itercnt+1) in [5,10,20,30,40,50, 60,80,100,120,150]:\n",
    "                print(\"========================= eval:\")\n",
    "                BB= EE.dot(FFt)\n",
    "                n100,r20,r50 = evaluate(BB)\n",
    "                del BB\n",
    "                print(\"[ %d,   %d,   %.3f,   %d,   %.3f,   %.3f,  %.3f ]\" %(dim, L2reg, prob_dropout, rhoadd, n100, r20, r50))\n",
    "                #print(\"%d %d\\t%.3f\\t%.3f\\t%.3f\" %(dim, L2reg, np.mean(n100_list), np.mean(r20_list), np.mean(r50_list)))\n",
    "\n",
    "                print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "123"
    }
   },
   "outputs": [],
   "source": [
    "[ 10,   5,   0.050,   500,   0.325,   0.289,  0.408 ]\n",
    "[ 20,   5,   0.050,   500,   0.356,   0.324,  0.447 ]\n",
    "[ 50,   10,   0.050,   500,   0.389,   0.358,  0.484 ] \n",
    "[ 100,   5,   0.050,   500,   0.405,   0.374,  0.503 ]\n",
    "[ 200,   5,   0.050,   500,   0.415,   0.382,  0.513 ]\n",
    "\n",
    "\n",
    "[ 50,   400,   0.250,   2000,   0.384,   0.348,  0.475 ]\n",
    "[ 50,   400,   0.200,   500,   0.386,   0.351,  0.479 ]\n",
    "[ 50,   400,   0.100,   500,   0.387,   0.354,  0.481 ]\n",
    "[ 50,   100,   0.100,   500,   0.388,   0.355,  0.482 ]\n",
    "[ 50,   30,   0.100,   500,   0.388,   0.355,  0.482 ]\n",
    "[ 50,   10,   0.100,   500,   0.388,   0.355,  0.483 ]\n",
    "[ 50,   10,   0.050,   500,   0.389,   0.358,  0.484 ]         ....best\n",
    "[ 50,   10,   0.020,   500,   0.389,   0.358,  0.483 ]\n",
    "[ 50,   10,   0.010,   500,   0.387,   0.355,  0.479 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "124"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(XtXdiag)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "125"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(betaTF, betaVec, 'x')\n",
    "plt.show()\n",
    "plt.plot(EEFFdiag, betaVec, 'x')\n",
    "plt.show()\n",
    "EEnn= (1.0 /np.sqrt(np.sum(EE*EE,1)))[:,None]  *EE\n",
    "FFtnn= FFt / np.sqrt(np.sum(FFt*FFt,0))\n",
    "cosineEDLAE=np.diag(EEnn.dot(FFtnn))\n",
    "del EEnn\n",
    "del FFtnn\n",
    "plt.hist(cosineEDLAE, bins=100, alpha=0.8,  density=True,histtype='step', color='black', linewidth=2.0,  label='calibrated')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "126"
    }
   },
   "outputs": [],
   "source": [
    "mm=dict()\n",
    "mm[\"betaVec\"]=-99#betaVec\n",
    "mm[\"etaVec\"] = -99#etaVec\n",
    "mm[\"EE\"]=EE\n",
    "mm[\"FFt\"]=FFt\n",
    "save_pkl(mm, \"/root/projects/mySLIM/paper/ml20m_AN_DLAE.pkl\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "127"
    }
   },
   "outputs": [],
   "source": [
    "mm = load_pkl( \"/root/projects/mySLIM/paper/msd_AN_edlae1.pkl\" )\n",
    "betaVec_orig = mm[\"betaVec\"]\n",
    "etaVec_orig = mm[\"etaVec\"] \n",
    "EE_orig = mm[\"EE\"]\n",
    "FFt_orig  = mm[\"FFt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "128"
    }
   },
   "outputs": [],
   "source": [
    "del mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "129"
    }
   },
   "outputs": [],
   "source": [
    "betaVec_orig=deepcopy(betaVec)\n",
    "etaVec_orig=deepcopy(etaVec)\n",
    "EE_orig=deepcopy(EE)\n",
    "FFt_orig=deepcopy(FFt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "130"
    }
   },
   "outputs": [],
   "source": [
    "# EDLAe with beta learning\n",
    "\n",
    "\n",
    "\n",
    "#print(\"precomputing\")\n",
    "#precompute\n",
    "#ii_diag=np.diag_indices(XtX.shape[0])\n",
    "#XtX[ii_diag]=L2reg+boost*XtXdiag  +XtXdiag \n",
    "#PP=np.linalg.inv(XtX)\n",
    "\n",
    "for rhoadd in [500.0 ]:\n",
    "  for prob_dropout in [  0.1]:\n",
    "    boost= prob_dropout/(1.0-prob_dropout)\n",
    "    print(\"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb prob_dropout= {}\".format(prob_dropout))\n",
    "    for L2reg in [  10.0 ]:\n",
    "      for dim in  [   1000 ] : #[ 10 , 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]:\n",
    "        print(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa dim={}\".format(dim))\n",
    "\n",
    "        \n",
    "        rhoVec = boost*XtXdiag  + L2reg +rhoadd\n",
    "        \n",
    "        betaVec= deepcopy(betaVec_orig)\n",
    "        etaVec= deepcopy(etaVec_orig)\n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "        EE=deepcopy(EE_orig)\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        print(\"precomputing\")\n",
    "        #precompute\n",
    "        ii_diag=np.diag_indices(XtX.shape[0])\n",
    "        XtX[ii_diag]= XtXdiag +boost*XtXdiag  + L2reg+ rhoVec\n",
    "        PP=np.linalg.inv(XtX)\n",
    "\n",
    "\n",
    "        for itercnt in range(20):\n",
    "            print(\"================= iterCnt: {}\".format(itercnt))\n",
    "            #print(\"  --- train FF\")\n",
    "            #print(datetime.datetime.now())\n",
    "            #### update FFt\n",
    "            ii_diag=np.diag_indices(XtX.shape[0])\n",
    "            XtX[ii_diag]= XtXdiag +boost*XtXdiag  + L2reg+ rhoVec\n",
    "            HH=EE.T.dot(XtX).dot(EE)\n",
    "            #print(\"          10\")\n",
    "            HH= np.linalg.inv(HH)  .dot(EE.T)\n",
    "            #print(\"          11\")\n",
    "            XtX[ii_diag]= XtXdiag\n",
    "            GG= XtX *(1.0+betaVec)\n",
    "            GG[ii_diag]+= rhoVec*  (betaVec-etaVec)\n",
    "            FFt= HH.dot(GG)\n",
    "            del GG\n",
    "            #######update EE\n",
    "            #print(\"          1\")\n",
    "            HH= FFt.dot(FFt.T)\n",
    "            HH=np.linalg.inv(HH)\n",
    "            HH=FFt.T.dot(HH)\n",
    "            #print(\"          3\")\n",
    "            XtX[ii_diag]= XtXdiag\n",
    "            GG= XtX *(1.0+betaVec)\n",
    "            GG[ii_diag]+= rhoVec*  (betaVec-etaVec)\n",
    "            EE= GG.dot(HH)\n",
    "            del GG\n",
    "            EE=PP.dot(EE)\n",
    "            ########## update betaVec\n",
    "            EEFFdiag  = np.diag(EE.dot(FFt))\n",
    "            if itercnt>0:\n",
    "                XtX[ii_diag]= XtXdiag\n",
    "                HH=np.diag(XtX.dot(EE).dot(FFt)) -XtXdiag +rhoVec* (etaVec+ EEFFdiag   )\n",
    "                GG=XtXdiag +rhoVec -boost*XtXdiag  - L2reg\n",
    "                betaVec=HH/GG\n",
    "                del GG\n",
    "                betaVec=np.maximum(betaVec, 0.0) # non-neg values !\n",
    "                if (itercnt+1) in [1,5,10,20,30,40]:\n",
    "                    plt.plot(betaTF, betaVec, 'x')\n",
    "                    plt.show()\n",
    "                    plt.plot(EEFFdiag, betaVec, 'x')\n",
    "                    plt.show()\n",
    "            if (itercnt+1) in [1,5,10,20,30,40]:\n",
    "                EEnn= (1.0 /np.sqrt(np.sum(EE*EE,1)))[:,None]  *EE\n",
    "                FFtnn= FFt / np.sqrt(np.sum(FFt*FFt,0))\n",
    "                cosineEDLAE=np.diag(EEnn.dot(FFtnn))\n",
    "                del EEnn\n",
    "                del FFtnn\n",
    "                plt.hist(cosineEDLAE, bins=100, alpha=0.8,  density=True,histtype='step', color='black', linewidth=2.0,  label='calibrated')\n",
    "                plt.show()\n",
    "            \n",
    "            ########## update etaVec\n",
    "            etaVec+= EEFFdiag-betaVec\n",
    "            \n",
    "            print(datetime.datetime.now())\n",
    "            ###### eval\n",
    "            if (itercnt+1) in [5,10,20,30,40,60,80,100]:\n",
    "                print(\"========================= eval:\")\n",
    "                BB= EE.dot(FFt)\n",
    "                evaluate(BB)\n",
    "                del BB\n",
    "                #print(\"%d %d\\t%.3f\\t%.3f\\t%.3f\" %(dim, L2reg, np.mean(n100_list), np.mean(r20_list), np.mean(r50_list)))\n",
    "\n",
    "                print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "131"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(EEFFdiag, betaVec, 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "132"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "local_vars = list(locals().items())\n",
    "for var, obj in local_vars:\n",
    "    print(var, sys.getsizeof(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "133"
    }
   },
   "outputs": [],
   "source": [
    "del GG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "134"
    }
   },
   "outputs": [],
   "source": [
    "del PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "135"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "136"
    }
   },
   "outputs": [],
   "source": [
    "EEnn= (1.0 /np.sqrt(np.sum(EE*EE,1)))[:,None]  *EE\n",
    "FFtnn= FFt / np.sqrt(np.sum(FFt*FFt,0))\n",
    "cosineEDLAE=np.diag(EEnn.dot(FFtnn))\n",
    "del EEnn\n",
    "del FFtnn\n",
    "plt.hist(cosineEDLAE, bins=100, alpha=0.8,  density=True,histtype='step', color='black', linewidth=2.0,  label='calibrated')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "137"
    }
   },
   "outputs": [],
   "source": [
    "#save_pkl(cosineEDLAE, \"/root/projects/mySLIM/paper/nflx_cosineANedlae.pkl\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "138"
    }
   },
   "outputs": [],
   "source": [
    "betaTF=load_pkl(\"/root/projects/mySLIM/paper/msd_betaTF.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "139"
    }
   },
   "source": [
    "# create figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "140"
    }
   },
   "outputs": [],
   "source": [
    "#mm = load_pkl( \"/root/projects/mySLIM/paper/msd_AN_edlae_best.pkl\" )\n",
    "mm = load_pkl( \"/root/projects/mySLIM/paper/msd_AN_DLAE.pkl\" )\n",
    "\n",
    "#mm = load_pkl( \"/root/projects/mySLIM/paper/ml20m_AN_EDLAE_learndiag.pkl\")\n",
    "#mm = load_pkl( \"/root/projects/mySLIM/paper/ml20m_AN_DLAE.pkl\" )\n",
    "\n",
    "#mm = load_pkl( \"/root/projects/mySLIM/paper/nflx_AN_edlaebest.pkl\")\n",
    "#mm = load_pkl( \"/root/projects/mySLIM/paper/nflx_AN_DLAE.pkl\")\n",
    "\n",
    "#betaVec_orig = mm[\"betaVec\"]\n",
    "#etaVec_orig = mm[\"etaVec\"] \n",
    "EE = mm[\"EE\"]\n",
    "FFt  = mm[\"FFt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "141"
    }
   },
   "outputs": [],
   "source": [
    "EEnn= (1.0 /np.sqrt(np.sum(EE*EE,1)))[:,None]  *EE\n",
    "FFtnn= FFt / np.sqrt(np.sum(FFt*FFt,0))\n",
    "#cosineEDLAE=np.diag(EEnn.dot(FFtnn))\n",
    "cosine_DLAE=np.diag(EEnn.dot(FFtnn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "142"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "143"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "144"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "145"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.hist(cosineEDLAE, bins=20, alpha=0.8,  density=True,histtype='step', color='black', linewidth=2.0,  label='calibrated')\n",
    "plt.hist(cosine_DLAE, bins=20, alpha=0.8,  density=True,histtype='step', color='blue', linewidth=2.0, linestyle='--', label='calibrated')\n",
    "#plt.plot(ml20_diag[:,0] , ml20_diag[:,2], 'o-k')\n",
    "#plt.plot(ml20_droppeddiag[:,0] , ml20_droppeddiag[:,2], '+--g')\n",
    "plt.xlabel('cosine')\n",
    "plt.ylabel('density')\n",
    "#plt.xscale('log')\n",
    "#plt.legend()\n",
    "#fig.savefig('/root/projects/mySLIM/paper/msd_cosine.pdf',  bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "146"
    }
   },
   "outputs": [],
   "source": [
    "# ml20============================================== diag enforced\n",
    "# dim   l2reg     ndcg    r20     r50\n",
    "\n",
    "ml20_diag=np.array([   [10, 2000 ,           0.284 ,   0.247  ,  0.354 ],\n",
    "[20, 2000 ,           0.327 ,   0.294  ,  0.409  ],\n",
    "[50, 500  ,          0.366 ,   0.333 ,   0.451 ],\n",
    "[100, 500 ,        0.388,   0.356 ,  0.478 ],\n",
    "[200, 500 ,     0.404,    0.373 ,   0.496 ],\n",
    "[500, 500 ,           0.414  ,  0.384  ,  0.512 ],\n",
    "[1000, 500,    0.417,    0.388 ,   0.516 ],\n",
    "[5000, 500,    0.420,    0.391 ,   0.521 ],\n",
    "[10000, 500,    0.420,    0.391  ,  0.521 ],\n",
    "[20108,   500,      0.420 ,  0.391 ,  0.521 ]     ])\n",
    "\n",
    "\n",
    "\n",
    "ml20_droppeddiag= np.array([   [10, 3000 ,    0.323 ,   0.290  ,  0.407],\n",
    "[20 ,10000 ,   0.350 ,   0.319  ,  0.435],\n",
    "[50 ,30000 ,   0.368 ,   0.337 ,   0.455],\n",
    "[100, 30000 ,   0.379 ,   0.351 ,   0.472],\n",
    "[200, 30000 ,   0.390 ,   0.362 ,   0.485],\n",
    "[500, 30000 ,   0.397 ,   0.365 ,   0.492],                          \n",
    "[1000 ,10000 ,   0.407,    0.377 ,   0.511],\n",
    "[5000 ,10000 ,   0.407 ,   0.376 ,   0.511],\n",
    "[10000, 10000 ,   0.407 ,   0.376,    0.511],\n",
    "[20108,  10000 ,    0.407,   0.376 ,  0.511]     ])\n",
    "\n",
    "# dim   l2reg  pdrop_in_L2xtx   rho_add_admm  ndcg     r20    r50\n",
    "ml20_learndiag= np.array([   [ 10,   5,   0.050,   500,   0.325,   0.289,  0.408 ],\n",
    "[ 20,   5,   0.050,   500,   0.356,   0.324,  0.447 ],\n",
    "[ 50,   10,   0.050,   500,   0.389,   0.358,  0.484 ], \n",
    "[ 100,   5,   0.050,   500,   0.405,   0.374,  0.503 ],\n",
    "[ 200,   5,   0.050,   500,   0.415,   0.382,  0.513 ],\n",
    "[ 500,   400,   0.333,   500,   0.416,   0.382,  0.515 ],\n",
    "[ 1000,   400,   0.333,   500,   0.418,   0.384,  0.516 ],\n",
    "[ 2000,   400,   0.333,   500,   0.419,   0.385,  0.518 ],\n",
    "[ 5000,   400,   0.333,   500,   0.419,   0.385,  0.518 ],\n",
    "[10000, 400,   0.333,   500,     0.420,    0.391  ,  0.521 ],\n",
    "[20108,   400,   0.333,   500,   0.420 ,  0.391 ,  0.521 ]  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "147"
    }
   },
   "outputs": [],
   "source": [
    "# nflx +++++++++++++++++++++++++++++++++++++++++++ diag enforced\n",
    "# dim   l2reg     ndcg     r20    r50\n",
    "\n",
    "nflx_diag=np.array([  [10 ,1000 ,    0.260 ,   0.225  ,  0.293],\n",
    "[20 ,1000 ,    0.291 ,   0.256  ,  0.331],\n",
    "[50 ,1000 ,    0.328 ,   0.296  ,  0.371],\n",
    "[100, 1000 ,   0.351 ,   0.319  ,  0.396],\n",
    "[200, 1000 ,   0.368 ,   0.338  ,  0.416],\n",
    "[500, 1000 ,   0.383 ,   0.353  ,  0.433],\n",
    "[1000, 1000 ,   0.389 ,   0.358 ,   0.439],\n",
    "[2000 ,1000 ,   0.392 ,   0.360  ,  0.443],\n",
    "[5000 ,1000 ,   0.393  ,  0.361  ,  0.445],\n",
    "[10000, 1000 ,   0.393 ,   0.362 ,   0.445],\n",
    "[17769, 1000 ,   0.393 ,   0.362  ,  0.445]  ])\n",
    "\n",
    "\n",
    "nflx_droppeddiag=np.array([ [10, 30000 ,   0.291  ,  0.259   , 0.336],\n",
    "[20, 30000 ,   0.319  ,  0.286   , 0.366],\n",
    "[50, 30000 ,   0.344  ,  0.311   , 0.389],\n",
    "[100, 100000,    0.350 ,   0.319 ,   0.397],\n",
    "[200, 100000,    0.355 ,   0.325 ,   0.405],  \n",
    "[500, 100000,    0.367 ,   0.336  ,  0.416],\n",
    "[1000, 100000,    0.369,    0.337  ,  0.418],\n",
    "[2000, 30000 ,   0.379 ,   0.346 ,   0.431],\n",
    "[5000, 30000 ,   0.380 ,   0.347 ,   0.431],\n",
    "[10000, 30000,    0.380 ,   0.346 ,   0.431],\n",
    "[17769, 30000,      0.380 ,   0.346  , 0.431]   ])\n",
    "\n",
    "# dim   l2reg  pdrop_in_L2xtx   rho_add_admm  ndcg     r20    r50\n",
    "nflx_learndiag=np.array([ [ 10,   30,   0.100,   500,   0.293,   0.260,  0.338 ],\n",
    "[ 20,   30,   0.100,   500,   0.320,   0.287,  0.368 ],\n",
    "[ 50,   30,   0.100,   500,   0.350,   0.317,  0.399 ] ,\n",
    "[ 100,   30,   0.100,   500,   0.367,   0.334,  0.417 ],\n",
    "[ 200,   30,   0.100,   500,   0.380,   0.347,  0.430 ],\n",
    "[ 500,   500,   0.333,   500,   0.388,   0.355,  0.438 ],\n",
    "[1000,   500,   0.333,   500,   0.392,   0.359,  0.443 ],\n",
    "[ 2000,   500,   0.333,   500,   0.393,   0.361,  0.445 ],\n",
    "[ 5000,   500,   0.333,   500,   0.393,   0.362,  0.445 ],\n",
    "[10000, 500,   0.333,   500,   0.393 ,   0.362 ,   0.445],\n",
    "[17769, 500,   0.333,   500,   0.393 ,   0.362  ,  0.445] \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "148"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# dim   l2reg     ndcg     r20    r50\n",
    "msd_diag=np.array([   [ 10, 100 ,   0.088  ,  0.066 ,   0.100],\n",
    "[20, 100 ,   0.106  ,  0.080 ,   0.118],\n",
    "[50 ,100 ,   0.142  ,  0.110 ,   0.157],\n",
    "[100, 100,    0.174 ,   0.138,    0.191],\n",
    "[200, 100 ,   0.211 ,   0.169 ,   0.231  ],\n",
    "[500 ,200 ,   0.263 ,   0.216 ,   0.290],\n",
    "[1000, 200 ,   0.302 ,   0.249,    0.333],\n",
    "[2000, 200 ,   0.336  ,  0.281 ,   0.370],\n",
    "[5000, 200 ,   0.368  ,  0.312  ,  0.406],\n",
    "[10000, 200 ,   0.381 ,   0.325 ,   0.420],\n",
    "[20000, 200,    0.388 ,   0.332 ,   0.427],\n",
    "[41140,   200 ,    0.389 ,   0.333 ,   0.428]    ])\n",
    "\n",
    "# dim   l2reg     ndcg     r20    r50\n",
    "msd_droppeddiag = np.array([   [10 ,1000,    0.097 ,   0.073  ,  0.109],\n",
    "[20 ,1000 ,   0.117 ,   0.089 ,   0.130 ],\n",
    "[50 ,30000 ,   0.142  ,  0.111 ,   0.158],\n",
    "[100, 30000 ,   0.167 ,   0.132 ,   0.184],\n",
    "[200, 30000 ,   0.195 ,   0.157  ,  0.216  ],\n",
    "[500, 30000 ,   0.239 ,   0.194  ,  0.264],\n",
    "[1000, 30000,    0.266 ,   0.217  ,  0.296],\n",
    "[2000, 5000 ,   0.292 ,   0.240  ,  0.328],\n",
    "[5000 ,5000 ,   0.328 ,   0.271  ,  0.367],\n",
    "[10000, 5000 ,   0.344 ,   0.285 ,   0.384],\n",
    "[20000, 5000 ,   0.347 ,   0.287 ,   0.388],\n",
    "[41140, 5000 ,      0.347 ,  0.284 ,  0.384 ]     ])\n",
    "\n",
    "# dim   l2reg  pdrop_in_L2xtx   rho_add_admm  ndcg     r20    r50\n",
    "msd_learndiag = np.array([ [ 10,   10,   0.150,   500,   0.099,   0.075,  0.111 ],\n",
    "[ 20,   10,   0.150,   500,   0.123,   0.093,  0.137 ],\n",
    "[ 50,   10,   0.150,   500.000,   0.160,   0.125,  0.179 ],\n",
    "[ 100,   10,   0.150,   500,   0.192,   0.152,  0.213 ],\n",
    "[200,  10, 0.1, 500,  0.22908, 0.18261,0.25379],\n",
    "[500,  10, 0.1, 500,  0.28053,0.22724,0.31061 ],\n",
    "[1000,  10, 0.1, 500,  0.31973,0.26288,0.26288 ],  \n",
    "[2000,  10, 0.1, 500,  0.35024, 0.29261 , 0.38471],\n",
    "[5000,  10, 0.1, 500, 0.37385, 0.31587 , 0.41165],\n",
    "[ 10000,   10,   0.100,   500.000,   0.381,   0.324,  0.421 ],\n",
    "[ 20000,   10,   0.100,   500.000,   0.388 ,  0.332,  0.427],\n",
    "[41140,   10,   0.100,   500.000,    0.389 ,   0.333 ,   0.428]    \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "149"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.plot(msd_diag[:,0] , msd_diag[:,2],  'x-g', linewidth=2.0)\n",
    "plt.plot(msd_droppeddiag[:,0] , msd_droppeddiag[:,2] , '+--b', linewidth=2.0)\n",
    "plt.plot(msd_learndiag[:,0] , msd_learndiag[:,4] , 'o-k', linewidth=2.0)\n",
    "plt.xlabel('matrix-rank')\n",
    "plt.ylabel('nDCG@100')\n",
    "plt.xscale('log')\n",
    "#plt.legend()\n",
    "fig.savefig('/root/projects/mySLIM/paper/msd_ranksweep.pdf',  bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "150"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.7 (to be deprecated)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  },
  "nterop": {
   "seedId": "150"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
